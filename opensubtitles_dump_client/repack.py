#! /usr/bin/env python3

# python -u opensubtitles_dump_client/repack.py | tee -a opensubtitles_dump_client/repack.py.log

# good signs:
# - no "keeping weird file name"
# - no "failed to verify file" (except nfo files, but we dont have checksums for nfo files)

# FIXME no file extension in file name

import os
import json
import subprocess
import glob
import sqlite3
import zipfile
import shutil
import tarfile
import lzma
import re
import hashlib
import time
import tempfile
import shlex

#import magic # libmagic # often wrong
import charset_normalizer

# english
lang_code = "eng"
lang_code_short = "en"
lang_code_charset_normalizer = "English"

"""
# czech
lang_code = "cze"
lang_code_short = "cs"
# no. the language-detection is often wrong
# often it returns a similar-but-wrong language:
# Czech, Slovak, Croatian
lang_code_charset_normalizer = "Czech"
"""

# charset_normalizer languages
# ['Czech', 'Slovak', 'Croatian', 'Dutch', 'Finnish', 'Spanish', 'Estonian', 'Portuguese', 'Italian', 'Hungarian', 'French', 'Swedish', 'Turkish', 'German', 'Romanian', 'Polish', 'Vietnamese']

# generated by index.py and index-compress.py
# TODO merge index-compress.py with index.py
#index_file = f"index-grouped/index.txt.grouped.{lang_code}"

opensubs_db_path = "opensubs.db"
metadata_db_path = "opensubs-metadata.db"
movie_names_db_path = "opensubs-movie-names.db"

opensubs_db_first_sub_number = 1
opensubs_db_last_sub_number = 9180518

convert_to_utf8 = False
convert_to_utf8 = True
verbose_recode_content = False
#verbose_recode_content = True

# txt files are not recognized as subtitles
# fix: rename to sub files
# https://github.com/mpv-player/mpv/issues/4144
rename_txt_to_sub = True

def is_sub_content(file_content):
    # example sub file:
    #   {1}{1}25.000
    #   {123}{234}first line|second line
    #   {345}{456}just one line
    #   {112269}{113769}...
    # the FPS line "{1}{1}25.000" is optional
    for line in file_content.split(b"\n"):
        # \s*: allow lines with only whitespace
        if not re.fullmatch(rb"\{\d+\}\{\d+\}.*|\s*", line):
            return False
    return True



debug_stop_after_n_groups = None
#debug_stop_after_n_groups = 1
#debug_stop_after_n_groups = 1000
debug_stop_after_n_groups = 10



# good for real-world users:
# have one movie, get all subs in one language
# bad for updates: old archives should be immutable

repack_strategy = "group-by-movie-name"



# actually this requires a first pass with group-by-movie-name
# and in the second pass, we can group-by-movie-name-hash

#repack_strategy = "group-by-movie-name-hash"
repack_strategy_hash_bits = 16 # hash size in bits



# bad for real-world users:
# subs for one movie are fragmented
# across many archives
# good for updates: old archives are immutable
# good for storage: old data is immutable
# good for compression: more files per archive than "group by movie name"

#repack_strategy = "group-by-sub-number"



repack_strategy = "group-by-imdb-id"



# how many files per archive?
# 1200 new subs per day are added to opensubtitles
# the first dump had 5719123 (6M) zipped subtitles in 127.70 GiB
# this means about 23.41 KiB per zipfile
# TODO uncompressed size
# bad compression: only 3% better than zip files, see readme.md
# 10 * 100 subs = 70 MB tempfiles
# 2 * 1000 subs = 140 MB tempfiles

repack_strategy_group_size = 100
repack_strategy_group_size = 10*1000
repack_strategy_group_size = 10 # debug
repack_strategy_group_size = 100 # xz is 40% smaller than zip
repack_strategy_group_size = 1000 # xz is 40% smaller than zip

debug_stop_after_n_groups = 2
debug_stop_after_n_groups = 1
debug_stop_after_n_groups = 100



# /run/user/1000/opensubs-repack
# NOTE tmpfs is limited by size of RAM
tempdir = f"/run/user/{os.getuid()}/opensubs-repack/{lang_code}" # tmpfs

output_dir = f"repack-grouped/{lang_code}/group-size-{repack_strategy_group_size}"
if repack_strategy == "group-by-imdb-id":
    output_dir = f"repack-grouped/{lang_code}/by-imdb-id"

cached_zip_dir = "done-zips"



# global state
#index_data = None
opensubs_db_connection = None
metadata_db_connection = None
movie_names_db_connection = None

opensubs_db_cursor = None
metadata_db_cursor = None

cached_zip_files = None

keep_tempdir = False
keep_tempdir = True # debug


def main():
    #global index_file
    #global index_data
    global metadata_db_path
    global metadata_db_connection
    global metadata_db_cursor
    global cached_zip_files
    global opensubs_db_connection
    global movie_names_db_connection
    global lang_code

    assert os.path.exists(opensubs_db_path), f"error: missing input file: {opensubs_db_path}"
    assert os.path.exists(metadata_db_path), f"error: missing input file: {metadata_db_path}"

    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(cached_zip_dir, exist_ok=True)

    #with open(index_file, "r") as f:
    #    index_data = json.load(f)

    metadata_db_connection = sqlite3.connect(metadata_db_path)
    # default: rows are tuples (faster)
    #metadata_db_connection.row_factory = sqlite3.Row # rows are dicts
    # metadata_data

    metadata_db_cursor = metadata_db_connection.cursor()
    #metadata_db_connection.row_factory = sqlite3.Row # rows are dicts
    metadata_db_cursor.row_factory = sqlite3.Row # rows are dicts

    cached_zip_files = glob.glob(f"{cached_zip_dir}/*.zip")
    cached_zip_files_by_num = dict()
    for zip_file in cached_zip_files:
        num = int(os.path.basename(zip_file).split(".", 2)[0])
        cached_zip_files_by_num[num] = zip_file
    cached_zip_files = cached_zip_files_by_num
    #print("cached zip files:", len(cached_zip_files))

    if repack_strategy == "group-by-movie-name":
        done_groups = 0
        sql_query = f"""
            select num, zipfile
            from subz_zipfiles
        """
        assert os.path.exists(movie_names_db_path), f"error: missing input file: {movie_names_db_path}"
        movie_names_db_connection = sqlite3.connect(movie_names_db_path)
        sql_cursor = movie_names_db_connection.cursor()
        # FIXME movie_name != group_name
        # movie_name: '"Abby Hatcher, Fuzzly Catcher" Otis on the Go/Abby\'s Afraid'
        # group_name: TODO use IMDB
        #for movie_name, movie_year in sql_cursor.execute(sql_query):
        for sub_num, sub_zipfile in sql_cursor.execute(sql_query):
            # no. we need the original movie_name for "where" clause in sql_query
            #movie_name = movie_name.strip()
            #print(movie_name, movie_year)
            repacked_movie = repack_by_movie_name(movie_name, movie_year, lang_code, lang_code_short)
            if repacked_movie == False:
                # dont count
                continue
            done_groups += 1
            if debug_stop_after_n_groups and done_groups >= debug_stop_after_n_groups:
                break
            print()

    elif repack_strategy == "group-by-sub-number":
        sql_query = "SELECT IDSubtitle FROM subz_metadata WHERE ISO639 = ? AND IDSubtitle BETWEEN ? AND ?"
        sql_args = (lang_code_short, opensubs_db_first_sub_number, opensubs_db_last_sub_number)
        sub_numbers = []
        done_groups = 0
        sql_cursor = metadata_db_connection.cursor()
        for (sub_number,) in sql_cursor.execute(sql_query, sql_args):
            #print("sub_number", sub_number)
            #if len(sub_numbers) >= 10: raise NotImplementedError
            sub_numbers.append(sub_number)
            if len(sub_numbers) >= repack_strategy_group_size:
                print("calling repack_by_sub_numbers")
                repack_by_sub_numbers(sub_numbers)
                sub_numbers = [] # start a new group
                done_groups += 1
                if debug_stop_after_n_groups and done_groups >= debug_stop_after_n_groups:
                    break
        # last group
        if len(sub_numbers) > 0:
            print("calling repack_by_sub_numbers")
            repack_by_sub_numbers(sub_numbers)

    elif repack_strategy == "group-by-movie-name-hash":
        #repack_strategy_hash_bits = 16 # hash size in bits
        raise NotImplementedError

    elif repack_strategy == "group-by-imdb-id":
        done_groups = 0
        # loop imdb ids of this language
        print(f"group-by-imdb-id: looping imdb ids of language {lang_code_short}")
        sql_query = "SELECT DISTINCT ImdbID FROM subz_metadata WHERE ISO639 = ?"
        sql_args = (lang_code_short,)
        sql_cursor = metadata_db_connection.cursor()
        for (imdb_id,) in sql_cursor.execute(sql_query, sql_args):
            print(f"group-by-imdb-id: imdb_id {imdb_id}")
            # filter garbage
            # FIXME this assumes that opensubs-metadata.db has valid imdb ids
            # it may give a better result to use the movie name and year
            # and do a lookup in the IMDB database
            # or at least use this way to verify the imdb ids
            if imdb_id == 0 or imdb_id > 30050816:
                continue
            # https://www.imdb.com/title/tt5573878/ # valid
            # https://www.imdb.com/title/tt0000001/ # valid
            # https://www.imdb.com/title/tt30050816/ # valid # last valid ID in opensubs-metadata.db
            # https://www.imdb.com/title/tt30979838/ # invalid # first invalid ID in opensubs-metadata.db
            # https://www.imdb.com/title/tt0000000/ # invalid
            # https://www.imdb.com/title/tt4294967295/ # invalid
            # https://www.imdb.com/title/tt1989193838419/ # invalid
            # https://www.imdb.com/title/tt1989193838419/ # invalid
            # loop sub numbers of this imdb id
            sql_cursor_2 = metadata_db_connection.cursor()
            # NOTE this may be slow
            # maybe add index f"idx_{table_name}_imdb_lang" in subtitles_all.txt.gz-parse.py
            sql_query = "SELECT IDSubtitle FROM subz_metadata WHERE ImdbID = ? AND ISO639 = ?"
            sql_args = (imdb_id, lang_code_short)
            # TODO? use sql_cursor_2.execute(sql_query, sql_args).fetchall()
            # FIXME this is slow. takes about 10 seconds
            # TODO add index to sqlite db
            t1 = time.time()
            print(f"group-by-imdb-id: looping sub_numbers of imdb id {imdb_id} ...")
            sub_numbers = []
            for (sub_number,) in sql_cursor_2.execute(sql_query, sql_args):
                sub_numbers.append(sub_number)
            t2 = time.time()
            print(f"group-by-imdb-id: looping sub_numbers of imdb id {imdb_id} done after {t2 - t1}sec")
            group_name = f"imdb-{imdb_id}-len-{len(sub_numbers)}"
            print("group-by-imdb-id: calling repack_by_sub_numbers")
            repack_by_sub_numbers(sub_numbers, group_name)
            done_groups += 1
            if debug_stop_after_n_groups and done_groups >= debug_stop_after_n_groups:
                break

# NOTE zipfiles can be multi-language
# sub 59918: english + romanian

def repack_by_sub_numbers(sub_numbers, group_name=None):
    sub_numbers.sort()
    if len(sub_numbers) == 0:
        print(f"repack_by_sub_numbers: no subs")
        return
    if not group_name:
        group_name = f"{sub_numbers[0]}-{sub_numbers[-1]}"
    print("group", group_name)
    group_tempdir = f"{tempdir}/{group_name}"
    #print("group_tempdir", group_tempdir)

    if os.path.exists(group_tempdir):
        content_size = path_getsize_recursive(group_tempdir)
        if content_size == 0:
            os.rmdir(group_tempdir)

    """
    sub_numbers_txt_path = f"{output_dir}/{group_name}.sub-numbers.txt"
    if os.path.exists(sub_numbers_txt_path):
        print("keeping", sub_numbers_txt_path)
    else:
        print("writing", sub_numbers_txt_path)
        with open(sub_numbers_txt_path, "w") as f:
            f.write("\n".join(map(str, sub_numbers)) + "\n")
    """

    original_zipfiles_decompress_speed = 0
    if os.path.exists(group_tempdir):
        print("keeping", group_tempdir)
    else:
        shutil.rmtree(group_tempdir, ignore_errors=True)
        os.makedirs(group_tempdir) # exist_ok=True
        # TODO use files in group_tempdir
        # to benchmark different compression algorithms

        #print("group_start", sub_numbers[0])

        """
        if os.path.exists(tar_xz_path):
            print(f"xz exists: {tar_xz_path}")
            return False
        """

        print(f"group {group_name}: unpacking {len(sub_numbers)} zipfiles")

        t1 = time.time()
        for sub_number in sub_numbers:
            # TODO why movie_name, movie_year
            #extract_movie_sub(sub_number, group_tempdir, movie_name, movie_year)
            extract_movie_sub(sub_number, group_tempdir)
        t2 = time.time()
        original_zipfiles_decompress_speed = t2 - t1

        #print("group_tempdir", group_tempdir)

    output_path_size_list = []

    # TODO get read time of uncompressed files
    content_size = path_getsize_recursive(group_tempdir)
    if content_size == 0:
        raise NotImplementedError(f"FIXME empty group_tempdir {group_tempdir}")
    output_path_size_list.append((group_tempdir, content_size, 0, 0))

    # create group of zipfiles so we can compare size and speed
    # NOTE this must run after all extract_movie_sub(sub_number, group_tempdir)
    # so cached_zip_files is populated
    output_path_original_zipfiles = f"{output_dir}/{group_name}.original-zipfiles"
    if os.path.exists(output_path_original_zipfiles):
        print("keeping", output_path_original_zipfiles)
    else:
        print("creating", output_path_original_zipfiles)
        os.makedirs(output_path_original_zipfiles)
        for sub_number in sub_numbers:
            if not sub_number in cached_zip_files:
                # FIXME missing zipfile
                continue
            zipfile_path = cached_zip_files[sub_number]
            os.symlink(os.path.abspath(zipfile_path), f"{output_path_original_zipfiles}/{os.path.basename(zipfile_path)}")

    # TODO also benchmark compression and decompression
    # for every compression algorithm
    # speed, cpu usage (multi core?), memory usage
    # decompression has more weight than compression
    # because we compress once and decompress often

    # TODO verify when tempdirs exist
    # so extract_movie_sub never runs
    # so cached_zip_files is not populated
    original_zipfiles_size = 0
    for sub_number in sub_numbers:
        if not sub_number in cached_zip_files:
            # FIXME missing zipfile
            continue
        zipfile_path = cached_zip_files[sub_number]
        original_zipfiles_size += os.path.getsize(zipfile_path)

    # TODO get decompress_time of original zipfiles
    output_path_size_list.append((output_path_original_zipfiles, original_zipfiles_size, 0, original_zipfiles_decompress_speed))

    """
    # create a sqlite database of the original zipfiles
    output_path_original_zipfiles_db = f"{output_dir}/{group_name}.original-zipfiles.db"
    if os.path.exists(output_path_original_zipfiles_db):
        print("keeping", output_path_original_zipfiles_db)
    else:
        print("writing", output_path_original_zipfiles_db)
        store_zipfiles_in_sqlite(output_path_original_zipfiles, output_path_original_zipfiles_db)
    output_path_size_list.append((output_path_original_zipfiles_db, os.path.getsize(output_path_original_zipfiles_db), 0, 0))
    """

    print(f"group {group_name}: compressing subs")

    # TODO create reproducible archives with python tarfile
    #compress_group_tempdir_in_memory(group_tempdir, tar_xz_path)
    #compress_group_in_file(group_tempdir, tar_xz_path)

    # TODO refactor compress_dir_tar and compress_dir_squashfs

    output_path_list = []

    # TODO create on *.tar file
    # and compress it with many compressors

    """
    # lzip: command not found
    # lzop: command not found
    # compress: command not found
    #for compressor in ["xz", "gz", "bz2", "lzip", "lzma", "lzop", "zstd", "compress"]:
    #for compressor in ["xz", "gz", "bz2", "lzma", "zstd"]:
    for compressor in ["xz"]:
        #for level in [5, 6, 7, 8, 9]:
        #for level in [7, 8, 9]:
        for level in [9]:
            output_path = f"{output_dir}/{group_name}.level{level}.tar.{compressor}"
            times = compress_dir_tar(group_tempdir, output_path, compressor, level) or [0, 0]
            output_path_list.append([output_path, *times])
    """

    # zstd
    # https://gitlab.com/gitlab-org/git/-/issues/89
    # If we were designing git today, it seems like a no-brainer to use zstd over zlib. 
    # Git should get a pretrained dictionary for better compression speed
    # https://linuxreviews.org/Comparison_of_Compression_Algorithms
    # zstd, appears to be the clear winner, with leading compression speed, decompression speed, and acceptable compression ratio.
    # zstd is also looking very good when the best compression level 19 and multiple cores are used.
    # Decompression is very fast and it is faster, not slower, when higher compression is used.
    # https://engineering.fb.com/2016/08/31/core-infra/smaller-and-faster-data-compression-with-zstandard/
    # lz4 aims for speed, xz aims for higher compression ratios, and zlib aims for a good balance of speed and size
    # As outlined, there are often drastic compromises between speed and size.
    # The fastest algorithm, lz4, results in lower compression ratios;
    # xz, which has the highest compression ratio, suffers from a slow compression speed.
    # However, Zstandard, at the default setting, shows substantial improvements
    # in both compression speed and decompression speed, while compressing at the same ratio as zlib.
    # https://github.com/inikep/lzbench/blob/master/lzbench18_sorted.md
    # zstd has fastest decompression
    # $ zstd --help
    #   --ultra                       Enable levels beyond 19, up to 22; requires more memory.
    # winner is zstd -9
    # zstd -12 looks interesting too, but for a 4% smaller compressed size
    # we would need 100% more compress time, which is a bad trade
    for compressor in ["zstd"]:
        #for level in [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]:
        #for level in [8, 9]: # just for fun
        for level in [9]:
            output_path = f"{output_dir}/{group_name}.level{level}.tar.{compressor}"
            times = compress_dir_tar(group_tempdir, output_path, compressor, level) or [0, 0]
            output_path_list.append([output_path, *times])

    # no. compressed size is 40% worse than with the default dictionary
    # i could spend more time to find a better dictionary, but... not now
    """
    # zstd with custom dictionary
    # $ zstd -9 -r --train /run/user/1000/opensubs-repack/ -o zstd-dictionary-level9
    compressor = "zstd"
    level = 9
    extra_compressor_args = ["-D", "/home/user/src/milahu/opensubtitles-scraper/zstd-dictionary-level9"]
    output_path = f"{output_dir}/{group_name}.level{level}.custom-dict.tar.{compressor}"
    times = compress_dir_tar(group_tempdir, output_path, compressor, level, extra_compressor_args=extra_compressor_args) or [0, 0]
    output_path_list.append([output_path, *times])
    """

    # this is really slow
    # $(which time) -v xz -z -9 --extreme -k -c imdb-96874-len-487.tar >imdb-96874-len-487.level9.extreme.tar.xz 
    # Elapsed (wall clock) time (h:mm:ss or m:ss): 3:24.34
    # Maximum resident set size (kbytes): 459868
    # output size: 5280

    # $(which time) -v xz -z -k imdb-96874-len-487.tar
    # Elapsed (wall clock) time (h:mm:ss or m:ss): 1:52.46
    # Maximum resident set size (kbytes): 97492
    # output size: 6304

    # zstd 9
    # zstd -9 -k --quiet -o imdb-96874-len-487.level9.tar.zstd imdb-96874-len-487.tar
    # bigger than xz: 10212 versus 6304

    # brotli 5
    # brotli -5 -k -o imdb-96874-len-487.level5.tar.br imdb-96874-len-487.tar
    # bigger than xz: 10400 versus 6304

    # brotli 9
    # brotli -9 -k -o imdb-96874-len-487.level9.tar.br imdb-96874-len-487.tar

    # ... or simply create a tar file and run turbobench
    # turbobench imdb-96874-len-487.tar

    # squashfs
    """
    for compressor in ["xz", "gzip", "lzma", "lzo", "lz4", "zstd"]:
        output_path = f"{output_dir}/{group_name}.{compressor}.sqfs"
        times = compress_dir_squashfs(group_tempdir, output_path, compressor) or [0, 0]
        output_path_list.append([output_path, *times])
    """

    # PPMd
    """
    # compression is 3x faster than xz at the same compressed size
    # decompression is 20x slower than xz at the same compressed size
    output_path = f"{output_dir}/{group_name}.s=on.yx=9.x=9.f=off.0=PPMd:mem2g:o32.7z"
    extra_args = ["-t7z", "-ms=on", "-myx=9", "-mx=9", "-mf=off", "-m0=PPMd:mem2g:o32"]
    times = compress_dir_7z(group_tempdir, output_path, extra_args) or [0, 0]
    output_path_list.append([output_path, *times])
    """

    # https://sigma-star.at/blog/2022/07/squashfs-erofs/
    # TODO mkfs.erofs --all-root -zxz repack-grouped/eng/group-size-1000/1-17839.xz.erofs /run/user/1000/opensubs-repack/1-17839/
    # TODO mkfs.erofs --all-root -zlz4_hc repack-grouped/eng/group-size-1000/1-17839.lz4_hc.erofs /run/user/1000/opensubs-repack/1-17839/

    for [output_path, compress_time, decompress_time] in output_path_list:
        output_path_size_list.append((output_path, os.path.getsize(output_path), compress_time, decompress_time))

    # sort by compressed size
    output_path_size_list.sort(key=lambda x: x[1])

    # print compression stats
    reference_size = original_zipfiles_size
    reference_size_2 = content_size
    """
    tar_xz_name = os.path.basename(tar_xz_path)
    print(f"tar {tar_xz_name}: raw {content_size / 1000:.0f}K = zips {original_zipfiles_size / 1000:.0f}K = xz {tar_xz_size / 1000:.0f}K = {smaller_percent:.0f}% smaller")
    """
    print(f"benchmark results for group {group_name}:")
    for (path, size, compress_time, decompress_time) in output_path_size_list:
        # 1048576 == 1024 * 1024
        compress_speed = 0 if compress_time == 0 else size / compress_time / 1048576 # MiB/s
        decompress_speed = 0 if decompress_time == 0 else size / decompress_time / 1048576 # MiB/s
        relative_size = round(size / reference_size * 100)
        relative_size_2 = round(size / reference_size_2 * 100)
        print(f"  {round(size / 1024):6d}   {relative_size:3d}%   {relative_size_2:3d}%   C:{compress_time:06.2f}sec={compress_speed:09.6f}MiB/s   D:{decompress_time:06.2f}sec={decompress_speed:09.6f}MiB/s   {path}")
        #print(f"  {round(size / 1024):6d}   {round(size / reference_size * 100):3d}%   C:{compress_time:06.2f}sec={compress_speed:09.6f}MiB/s   D:{decompress_time:06.2f}sec={decompress_speed:09.6f}MiB/s   {path}")
        #print(f"  {round(size / 1024):6d}   {round(size / reference_size * 100):3d}%   C:{compress_speed:05.2f}MiB/s   D:{decompress_speed:05.2f}MiB/s   {path}")
        #print(f"  {round(size / 1024):6d}   {round(size / reference_size * 100):3d}%   C:{compress_time:06.2f}sec   D:{decompress_time:06.2f}sec   {path}")

    # cleanup
    if not keep_tempdir:
        shutil.rmtree(group_tempdir, ignore_errors=True)



def store_zipfiles_in_sqlite(output_path_original_zipfiles, output_path_original_zipfiles_db):
    # based on new-subs-archive.py
    con = sqlite3.connect(output_path_original_zipfiles_db)
    cur = con.cursor()
    table_name = "zipfiles"
    cur.execute("PRAGMA count_changes=OFF")
    cur.execute(
        f"CREATE TABLE {table_name} (\n"
        f"  num INTEGER PRIMARY KEY,\n"
        f"  name TEXT,\n"
        f"  content BLOB\n"
        f")"
    )
    sql_query = f"INSERT INTO {table_name} VALUES (?, ?, ?)"
    sum_files = list(glob.glob(f"{output_path_original_zipfiles}/*.zip"))
    sum_files.sort()
    for file_path in sum_files:
        file_name = os.path.basename(file_path)
        name_parts = file_name.split(".")
        # note: name_parts[0] is zero-padded with format "%09d"
        num = int(name_parts[0])
        assert name_parts[-1] == "zip", f"not a zip file: {file_path}"
        # check for legacy file format before new-subs-rename-remove-num-part.py
        # FIXME fix filename format of zipfiles in done-zips/
        # remove ".(12345)" from "000012345.some.movie.(12345).zip"
        #assert name_parts[-2] != f"({num})", f"bad filename format: {file_path}"
        if name_parts[-2] == f"({num})":
            name_parts.pop() # "zip"
            name_parts.pop() # f"({num})"
            name_parts.append("zip")
        name = ".".join(name_parts[1:-1])
        with open(file_path, "rb") as f:
            content = f.read()
        sql_args = (num, name, content)
        cur.execute(sql_query, sql_args)
    con.commit()
    con.close()



def filter_tarinfo(tarinfo):
    # reset file owner to zero
    # TODO reset file time
    # TODO numeric owner
    tarinfo.uid = tarinfo.gid = 0
    tarinfo.uname = tarinfo.gname = "root"
    return tarinfo


def repack_by_movie_name(movie_name, movie_year, lang_code, lang_code_short):
    if False:
        print(f"movie_name", movie_name)
        print(f"movie_year", movie_year)
        raise NotImplementedError

    if movie_name.strip() == "":
        print(f"FIXME empty movie name")
        return False

    group_name = f"""{movie_name.strip().replace(" ", ".")}.{movie_year}.{lang_code}""" # alien.3.1992.eng
    # TODO remove parens from group_name?
    # TODO remove multiple dots from group_name? re.replace(r"\.{2,}", ".", group_name)
    # dir camel(s).2001.eng: ()
    # dir va.savoir.(who.knows).2001.eng: ()
    # dir on_line.2002.eng: _
    # dir Az.önvédelem.nagymestere.1983.eng: utf8
    if False:
        assert (
            re.fullmatch(r"[a-z0-9.()_]+\.\d{1,4}\." + lang_code, group_name)
        ), (
            f"invalid group name: {group_name}"
        )

    sub_numbers = []

# TODO



    # TODO advanced duplicate detection
    # when two subs have the exact same text
    # only with a small offset (like 1 second)
    # then keep only one sub (the more popular one)
    # subtitle sync can be automated with tools like ffsync
    # but probably this gives only about 5% smaller size of archive
    # so its not worth the complexity

    #movie_subs = index_data[movie_name_year]
    # TODO MovieKind = "tv"
    # debug
    print(
        f"""select IDSubtitle, MovieKind from subz_metadata """
        f"""where MovieName = {repr(movie_name)} and MovieYear = {movie_year} """
        f"""and ISO639 = "{lang_code_short}" and MovieKind = "movie";"""
    )
    sql_query = """
    select IDSubtitle, MovieKind
    from subz_metadata
    where MovieName = ? and MovieYear = ? and ISO639 = ?
    and MovieKind = "movie"
    """
    sql_args = (movie_name, movie_year, lang_code_short)
    sql_cursor = metadata_db_connection.cursor()
    num_subs = 0
    #for sub_number, _sub_numparts, _sub_positions, _crc16 in movie_subs:
    #metadata_db_cursor.row_factory = sqlite3.Row # rows are dicts
    for sub_number, sub_MovieKind in sql_cursor.execute(sql_query, sql_args):
        #print(f"sub {group_name} {sub_number}: sub_MovieKind {sub_MovieKind}")
        if sub_MovieKind == "tv": # TODO remove
            print(f"sub {group_name} {sub_number}: TODO implement MovieKind=tv")
            # TODO reduce number of archives by grouping seasons of TV shows
            # inside the archive, create one folder per episode
            # use strict format for season numbers and episode numbers
            # s01.e01
            # s01e01
            # season1.episode1
            continue
        sub_numbers.append(sub_number)
        num_subs += 1

    print("calling repack_by_sub_numbers")
    return repack_by_sub_numbers(sub_numbers, group_name)



# https://stackoverflow.com/a/1392549/10440128
def path_getsize_recursive(start_path = '.'):
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(start_path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            # skip if it is symbolic link
            if not os.path.islink(fp):
                total_size += os.path.getsize(fp)

    return total_size


def compress_group_tempdir_in_memory(group_tempdir, tar_xz_path):
    """
    create tar.xz file in memory
    """
    global tempdir
    # TODO sort files to get reproducible archive
    # https://reproducible-builds.org/docs/archives/
    #tar_xz_path = f"{tempdir}/{group_name}.py.tar.xz"
    #print(f"writing {tar_xz_path}")
    with tarfile.open(tar_xz_path, "w:xz") as tar:
        # TODO sort files to get reproducible archive
        for sub_dir in os.listdir(group_tempdir):
            if os.path.isfile(sub_dir):
                continue
            if sub_dir == "split-archives": continue # TODO remove
            tar.add(f"{group_tempdir}/{sub_dir}", sub_dir, filter=filter_tarinfo)
    #print(f"writing {tar_xz_path} done")

    # 453120  expected size
    # 467912  /run/user/1000/alien 3 (1992).tar.xz # convert_to_utf8=True
    # 467248  /run/user/1000/alien 3 (1992).tar.xz # convert_to_utf8=False
    # blame utf8?


def compress_group_in_file(group_tempdir, tar_xz_path):
    """
    create tar.xz file from tar file
    """
    tar_path = tar_xz_path[0:-3] # remove ".xz"
    print(f"writing {tar_path} ...")
    # format=tarfile.GNU_FORMAT gives smaller files
    # https://stackoverflow.com/questions/73643142/large-size-of-the-tar-files-generated-by-python-tarfile
    with tarfile.open(tar_path, "w", format=tarfile.GNU_FORMAT) as tar:
        # TODO sort files to get reproducible archive
        # https://reproducible-builds.org/docs/archives/
        for sub_dir in os.listdir(group_tempdir):
            if os.path.isfile(sub_dir):
                continue
            if sub_dir == "split-archives": continue # TODO remove
            tar.add(f"{group_tempdir}/{sub_dir}", sub_dir, filter=filter_tarinfo)
    print(f"writing {tar_path} done")
    print(f"writing {tar_xz_path} ...")
    with (
        open(tar_path, "rb") as src,
        lzma.open(tar_xz_path, "w", preset=6) as dst
    ):
        dst.write(src.read())
    print(f"writing {tar_xz_path} done")


def compress_dir_tar(input_path, output_path, compressor="xz", level=None, extra_args=[], extra_compressor_args=[]):
    if os.path.exists(output_path):
        print(f"keeping {output_path}")
        return
        #os.unlink(output_path)
    else:
        print(f"writing {output_path}")
    # TODO atomic write
    # first write to a temporary file on the same filesystem as output_path
    # and when complete, move it to output_path
    assert output_path.endswith(f".tar.{compressor}"), f"expected suffix .tar.{compressor} in file path: {output_path}"
    tar_path = output_path[0:-3] # remove ".xz"
    group_name = os.path.basename(input_path)
    #print(f"writing {output_path} ...")
    tar_files = []
    tar_cwd = None
    if False:
        # store multiple folders in the archive
        for sub_dir in os.listdir(input_path):
            if os.path.isfile(sub_dir):
                continue
            if sub_dir == "split-archives": continue # TODO remove
            tar_files.append(sub_dir)
        # tar_files are relative to input_path
        tar_cwd = input_path
    else:
        # store one folder in the archive
        tar_files = [os.path.basename(input_path)]
        # tar_files are relative to parent dir of input_path
        tar_cwd = os.path.dirname(input_path)
    # need absolute path because we change cwd
    output_path_abs = os.path.abspath(output_path)

    """
    compressor_arg_of_compressor = {
        "xz": "--xz", # compress with xz (default settings: level 6, no --extreme)
        "gz": "--gzip",
        "bz2": "--bzip2",
        "lzip": "--lzip",
        "lzma": "--lzma",
        "lzop": "--lzop",
        "zstd": "--zstd",
        "compress": "--compress",
    }
    compressor_arg = compressor_arg_of_compressor[compressor]
    """

    compressor_cmd_of_compressor = {
        "xz": "xz",
        "gz": "gzip",
        "bz2": "bzip2",
        "lzip": "lzip",
        "lzma": "lzma",
        "lzop": "lzop",
        "zstd": "zstd",
        "compress": "compress",
    }
    compressor_cmd = compressor_cmd_of_compressor[compressor]

    if level:
        extra_compressor_args = [f"-{level}"] + extra_compressor_args

    compressor_args = [compressor_cmd] + extra_compressor_args

    print("compressor_args:", shlex.join(compressor_args))

    compressor_arg = f"--use-compress-program={shlex.join(compressor_args)}"

    t1 = time.time()
    subprocess.run(
        [
            "tar",
            # all these options are required to create reproducible archives
            # https://reproducible-builds.org/docs/archives/
            # TODO create reproducible archives with python tarfile
            # so this also works on windows
            "--format=gnu",
            "--sort=name", # sort filenames, independent of locale. tar v1.28
            "--mtime=0",
            "--owner=0",
            "--group=0",
            "--numeric-owner",
            "-c", # create archive
            compressor_arg,
            *extra_args,
            "-f", output_path_abs, # output file
        ] + tar_files, # input files
        cwd=tar_cwd,
        check=True,
    )
    t2 = time.time()
    compress_time = t2 - t1
    # decompress
    # TODO pass extra_args
    #if compressor == "zstd" and len(extra_args) > 0:
    #    raise NotImplementedError("TODO pass extra_args to zstd decompress")
    t1 = time.time()
    subprocess.run(
        [
            "tar",
            "-x", # extract archive
            "--to-stdout",
            compressor_arg,
            "-f", output_path,
        ],
        check=True,
        stdout=subprocess.DEVNULL,
    )
    t2 = time.time()
    decompress_time = t2 - t1
    times = [compress_time, decompress_time]
    return times
    #print(f"writing {output_path} done")

    if False:
        # run xz separately to compress .tar to .tar.xz
        # this gives the exact same result as "tar --xz"
        print(f"writing {output_path} ...")
        subprocess.run(
            [
                "xz",
                #"-k", # keep input file
                tar_path,
            ],
            check=True,
        )
        print(f"movie {group_name}: writing {output_path} done")



def compress_dir_squashfs(input_path, output_path, compressor="xz"):
    if os.path.exists(output_path):
        print(f"keeping {output_path}")
        return
        #os.unlink(output_path)
    else:
        print(f"writing {output_path}")
    assert output_path.endswith(f".{compressor}.sqfs"), f"expected suffix .{compressor}.sqfs in file path: {output_path}"
    #print(f"writing {output_path} ...")
    pack_file = tempfile.mktemp()
    with open(pack_file, "w") as f:
        lines = [
            f"glob / 0555 0 0 -type d {input_path}",
            f"glob / 0444 0 0 -type f {input_path}",
        ]
        f.write("\n".join(lines) + "\n")
    args = [
        "gensquashfs",
        "--quiet",
        "--compressor", compressor,
        "--all-root",
        "--pack-file", pack_file,
        "--pack-dir", input_path,
        output_path
    ]
    t1 = time.time()
    subprocess.run(
        args,
        check=True,
    )
    t2 = time.time()
    compress_time = t2 - t1
    # TODO this is non-trivial
    # we must mount the sqfs filesystem as root
    # and then read files from the mountpoint
    decompress_time = 0
    """
    # decompress
    t1 = time.time()
    subprocess.run(
        [
            "7z",
            "x", # extract archive
            "-so", # extract to stdout
            output_path,
        ],
        check=True,
        stdout=subprocess.DEVNULL,
    )
    t2 = time.time()
    decompress_time = t2 - t1
    """
    times = [compress_time, decompress_time]
    os.unlink(pack_file)
    return times
    #print(f"writing {output_path} done")



def compress_dir_7z(input_path, output_path, extra_args):
    # 7z a -t7z -ms=on -myx=9 -mx=9 -mf=off -m0=PPMd:mem2g:o32 1-17839.s=on.yx=9.x=9.f=off.0=PPMd:mem2g:o32.7z /run/user/1000/opensubs-repack/1-17839/
    if os.path.exists(output_path):
        print(f"keeping {output_path}")
        return
        #os.unlink(output_path)
    else:
        print(f"writing {output_path}")
    assert output_path.endswith(".7z"), f"expected suffix .7z in file path: {output_path}"
    #print(f"writing {output_path} ...")
    args = [
        "7z",
        "a",
    ]
    args += extra_args
    args += [
        output_path,
        input_path,
    ]
    t1 = time.time()
    subprocess.run(
        args,
        check=True,
    )
    t2 = time.time()
    compress_time = t2 - t1
    # decompress
    t1 = time.time()
    subprocess.run(
        [
            "7z",
            "x", # extract archive
            "-so", # extract to stdout
            output_path,
        ],
        check=True,
        stdout=subprocess.DEVNULL,
    )
    t2 = time.time()
    decompress_time = t2 - t1
    times = [compress_time, decompress_time]
    return times
    #print(f"writing {output_path} done")



# TODO erofs
# mkfs.erofs --all-root -zlz4 repack-grouped/eng/group-size-1000/1-17839.lz4.erofs /run/user/1000/opensubs-repack/1-17839/
# bad compression


def sha1sum(file_path):
    # https://stackoverflow.com/questions/22058048/hashing-a-file-in-python
    # BUF_SIZE is totally arbitrary, change for your app!
    BUF_SIZE = 65536  # lets read stuff in 64kb chunks!
    #md5 = hashlib.md5()
    sha1 = hashlib.sha1()
    with open(file_path, 'rb') as f:
        while data := f.read(BUF_SIZE):
            #md5.update(data)
            sha1.update(data)
    return sha1.digest()
    #print("SHA1: {0}".format(sha1.hexdigest()))



#def extract_movie_sub(sub_number, group_tempdir, movie_name, movie_year):
def extract_movie_sub(sub_number, group_tempdir):
    # TODO split this function
    # TODO recode in a separate function (convert_to_utf8)
    # TODO test recode
    # TODO validate zip archive? (how? no checksums)
    # TODO validate extracted files? (CRC32 in zip metadata)
    # TODO remove parens from group_name

    global cached_zip_files
    global cached_zip_dir
    global opensubs_db_cursor

    group_name = os.path.basename(group_tempdir)

    # get the zip file
    if sub_number in cached_zip_files:
        zip_path = cached_zip_files[sub_number] # example: ../done-zips/000000123.pulp.fiction.zip
        zip_name = os.path.basename(zip_path).split(".", 1)[1] # example: pulp.fiction.zip
        zip_basename = zip_name[0:-4] # example: pulp.fiction
    else:
        if not opensubs_db_cursor:
            # connect only on demand
            opensubs_db_connection = sqlite3.connect(opensubs_db_path)
            # default: rows are tuples
            opensubs_db_connection.row_factory = sqlite3.Row # rows are dicts
            opensubs_db_cursor = opensubs_db_connection.cursor()
        #print(f"getting zipfile number {sub_number}")
        opensubs_db_cursor.execute("select * from subz where num = (?)", (sub_number,))
        row = opensubs_db_cursor.fetchone()
        if row == None:
            # $ sqlite3 opensubs-metadata.db "select * from subz_metadata where IDSubtitle = 504"
            # 504|Bad Education|2004|English|en|2005-04-14 02:35:02|275491|srt|1|bad education|0.0|0|0|0|movie|http://www.opensubtitles.org/subtitles/504/bad-education-en
            # $ sqlite3 opensubs.db "select name from subz where num = 504" | wc -l 
            # 0
            # FIXME some subs are missing in opensubs.db
            #raise NotImplementedError(f"FIXME zipfile not found for sub_number {sub_number}")
            print(f"FIXME zipfile not found for sub_number {sub_number}")
            return False
        #print("row", repr(row)[0:200], "...")
        zip_name = row["name"][22:-1] # example: pulp.fiction.zip
        zip_basename = zip_name[0:-4] # example: pulp.fiction
        zip_path = f"{cached_zip_dir}/{sub_number:09d}.{zip_name}" # example: ../done-zips/000000123.pulp.fiction.zip
        cached_zip_files[sub_number] = zip_path
        #print(f"writing zipfile {zip_path}")
        # atomic write
        with open(zip_path + ".tmp", "wb") as f:
            f.write(row["file"])
        os.rename(zip_path + ".tmp", zip_path)

    # get metadata for this sub
    # TODO? pass sub_metadata to extract_movie_sub and repack_by_sub_numbers
    sql_query = "select SubFormat from subz_metadata where IDSubtitle = (?)"
    sql_args = (sub_number,)
    (sub_metadata_subs_format,) = metadata_db_cursor.execute(sql_query, sql_args).fetchone()
    # force lowercase
    sub_metadata_subs_format = sub_metadata_subs_format.lower()
    if sub_metadata_subs_format == "txt":
        sub_metadata_subs_format = "sub"

    # sub_dirname is not used
    # we just need sub_num_parts
    #sub_num_parts = "1cd" # SubSumCD in subtitles_all.txt
    # get the sub name
    # 10: remove padded sub number with dot. example: "000000001."
    # -4: remove ".zip" file extension
    sub_dirname = os.path.basename(zip_path)[10:-4]
    #print("sub_dirname", repr(sub_dirname))
    sub_num_parts = sub_dirname.split(".")[-2]
    assert re.fullmatch(r"\d+cd", sub_num_parts)

    if False:
        # get the sub name
        # 10: remove padded sub number with dot. example: "000000001."
        # -4: remove ".zip" file extension
        sub_dirname = os.path.basename(zip_path)[10:-4]
        print("sub_dirname", repr(sub_dirname))

        # os.path.basename(zip_path) # "000000001.alien.3.(1992).eng.2cd.(1).zip"
        # sub_dirname # "alien.3.(1992).eng.2cd.(1)"
        # movie_name_year_original # "alien.3.(1992)"
        # movie_name # "alien 3"
        # movie_year # "1992"

        sub_dirname_prefix = f"{movie_name_year_original}.{lang_code}." # alien.3.(1992).eng.
        print("sub_dirname_prefix", repr(sub_dirname_prefix))
        assert (
            sub_dirname.startswith(sub_dirname_prefix)
        ), (
            f"sub_dirname {repr(sub_dirname)} must start with {repr(sub_dirname_prefix)}"
        )

        sub_dirname_rest = sub_dirname[len(sub_dirname_prefix):] # 2cd.(1)
        print("sub_dirname_rest", repr(sub_dirname_rest))
        assert re.fullmatch(r"\d+cd\.\(\d+\)", sub_dirname_rest), f"sub_dirname_rest {repr(sub_dirname_rest)}"

        sub_dirname = f"{movie_name_year_original}.{lang_code}." # alien.3.(1992).eng.

        #group_name = f"""{movie_name.replace(" ", ".")}.{movie_year}.{lang_code}""" # alien.3.1992.eng
    if False:
        # simple
        # sub_dirname_prefix is in group_name
        # 1cd/2cd/... is "1cd" for 98% of all subs, so its not relevant
        # not used
        sub_dirname = str(sub_number)

    # TODO remove parens from year and sub_number
    # a: alien.3.(1992).eng.1cd.(114195)
    # b: alien.3.1992.eng.1cd.114195
    # TODO use a strict format for group_name. see also: index.txt.outliers
    # "strict" means: easy to parse
    # TODO remove ".1cd" ".2cd" etc. 98% of all subs are 1cd
    # a: alien.3.(1992).eng.1cd.(114195)
    # b: alien.3.1992.eng.114195
    # TODO? add prefix "os" before sub_number
    # a: alien.3.(1992).eng.1cd.(114195)
    # b: alien.3.1992.eng.os114195
    if False:
        # move the sub_number to the front of the dirname
        # a: kamikaze.girls.(2004).eng.1cd.(237742)
        # b: 237742.kamikaze.girls.(2004).eng.1cd
        # remove number suffix. example: ".(1234)"
        sub_dirname = sub_dirname[0:(-1 * len(f".({sub_number})"))]
        # add number prefix. example: "1234.". not zero-padded -> use numeric sort for tar
        sub_dirname = f"{sub_number}.{sub_dirname}"
        #print("sub_dirname", sub_dirname)

    # TODO remove the movie name prefix?
    # a: kamikaze.girls.(2004).eng.1cd.(237742)
    # b: 1cd.(237742)
    # no. keep it simple. just keep the original name

    # now sorting is useless because "1cd" comes before the sub number
    # example:
    # the.curse.of.the.jade.scorpion.(2001).eng.1cd.(109215)/
    # the.curse.of.the.jade.scorpion.(2001).eng.1cd.(96132)/

    # not used. see below (sub_tempdir)
    if False:
        sub_tempdir = f"{group_tempdir}/{sub_dirname}"
        os.makedirs(sub_tempdir) # exist_ok=True
        #print("sub_tempdir", sub_tempdir)

    with zipfile.ZipFile(zip_path, 'r') as sub_zip:
        # infolist: Return a list containing a ZipInfo object for each member of the archive.
        # The objects are in the same order as their entries in the actual ZIP file on disk if an existing archive was opened.
        # Note: The open(), read() and extract() methods can take a filename or a ZipInfo object.
        # You will appreciate this when trying to read a ZIP file that contains members with duplicate names.
        seen_filepaths = set()

        # TODO test

        # FIXME fix weird chars in filename?
        # alien.3.1992.eng/5980562/Alien┬│.en.srt
        # alien.3.1992.eng/6029152/Alien┬│.Directors.Cut.1992.1080p.BRrip.x264.GAZ.YIFY.srt

# TODO why is this so slow when all output files exit?
# slow database queries? should we use more cursors?

        # guess encoding of file names
        # Note: The ZIP file standard historically did not specify a metadata encoding,
        # but strongly recommended CP437 (the original IBM PC encoding) for interoperability.
        # Recent versions allow use of UTF-8 (only).
        # FIXME different files can have different filename encodings

        #metadata_encoding = magic.detect_from_content("\n".join(sub_zip.namelist())).encoding
        # use charset_normalizer
        sub_zip_namelist_str = "\n".join(sub_zip.namelist()) + "\n"
        # assumption: zipfile decodes filenames with utf8 by default
        sub_zip_namelist_bytes = sub_zip_namelist_str.encode("utf8")
        metadata_charset_matches = charset_normalizer.from_bytes(sub_zip_namelist_bytes)
        metadata_charset_best_match = metadata_charset_matches.best()
        metadata_encoding = metadata_charset_best_match.encoding

        """
        #if metadata_encoding != "ascii" or metadata_charset_best_match.language != "English":
        if metadata_charset_best_match.language != "English":
            print("sub_zip_namelist: sub_zip_namelist_bytes", repr(sub_zip_namelist_bytes))
            print("sub_zip_namelist: metadata_charset_matches", metadata_charset_matches)
            #print("sub_zip_namelist: metadata_charset_best_match", metadata_charset_best_match)
            print("sub_zip_namelist: metadata_charset_best_match.could_be_from_charset", metadata_charset_best_match.could_be_from_charset)
            print("sub_zip_namelist: metadata_charset_best_match.encoding", metadata_charset_best_match.encoding)
            print("sub_zip_namelist: metadata_charset_best_match.language", metadata_charset_best_match.language)
            print("sub_zip_namelist: metadata_charset_best_match.languages", metadata_charset_best_match.languages)
        """

        #if metadata_encoding != "binary":
        if not metadata_encoding in {"ascii"}:
            print(f"sub {group_name} {sub_number}: TODO verify: metadata_encoding {metadata_encoding} from sub_zip_namelist_bytes", repr(sub_zip_namelist_bytes))

        # find the nfo file
        nfo_file_content = ""
        for zip_info in sub_zip.infolist():
            # TODO be more specific? see below: multiple nfo files
            if not zip_info.filename.endswith(f".({sub_number}).nfo"):
                continue
            if nfo_file_content != "":
                print(f"sub {group_name} {sub_number}: found rare edge-case: multiple nfo files")
                break
            with sub_zip.open(zip_info, "r") as src:
                nfo_file_content = src.read()
            while True:
                try:
                    nfo_file_content = nfo_file_content.decode("utf8")
                    break
                except UnicodeDecodeError as err:
                    print(
                        f"sub {sub_number}: found rare edge-case: "
                        f"invalid utf8 in the nfo file {repr(zip_info.filename)}: {err}. "
                        f"replacing broken bytes with null bytes"
                    )
                    # sub 8490893: found rare edge-case: invalid utf8 in the nfo file 'there.s.something.about.mary.(8490893).nfo': 'utf-8' codec can't decode byte 0xc2 in position 4037: invalid continuation byte. replacing broken bytes with null bytes
                    # replace broken bytes with null bytes
                    # later remove null bytes with rstrip("\x00")
                    nfo_file_content_2 = (
                        nfo_file_content[0:err.start] +
                        b"\x00" * (err.end - err.start) +
                        nfo_file_content[err.end:]
                    )
                    if False:
                        print(f"err.start: {err.start}")
                        print(f"err.end: {err.end}")
                        print(f"nfo_file_content at error +-4bytes: {repr(nfo_file_content[err.start - 4:err.end + 4])}")
                        print(f"nfo_file_content_2 at error +-4bytes: {repr(nfo_file_content[err.start - 4:err.end + 4])}")
                        print(f"nfo_file_content_2[err.start]: {repr(nfo_file_content_2[err.start])}")
                        print(f"nfo_file_content_2[err.start - 1]: {repr(nfo_file_content_2[err.start - 1])}")
                        print(f"nfo_file_content_2[err.start + 1]: {repr(nfo_file_content_2[err.start + 1])}")
                        assert len(nfo_file_content) == len(nfo_file_content_2)
                    nfo_file_content = nfo_file_content_2
                    if False:
                        print("""                    b"\\x00": """ + repr(b"\x00"))
                        print(f"nfo_file_content[err.start]: {repr(nfo_file_content[err.start])}")
                        print("""                    b"\\x00": """ + "%02x".format(b"\x00"))
                        print(f"nfo_file_content[err.start]: {nfo_file_content[err.start]:02x}")
                        print(f"nfo_file_content[err.start - 1]: {repr(nfo_file_content[err.start - 1])}")
                        print(f"nfo_file_content[err.start + 1]: {repr(nfo_file_content[err.start + 1])}")
                        print(f"nfo_file_content at error +-4bytes: {repr(nfo_file_content[err.start - 4:err.end + 4])}")
                    #assert nfo_file_content[err.start] == b"\x00"[0]
                    assert nfo_file_content[err.start] == 0

        # parse file names, sizes and hashes from the nfo file
        # the file names are cut after 57 bytes
        # note: 57 bytes, not 57 utf8 chars
        # this is a bug in the nfo generator
        # Û  Filename       : 123456789012345678901234567890123456789012345678901234567 Û
        zip_file_integrity_data = []
        nfo_file_filename = None
        class IntegrityData:
            # maximum length is 57 bytes
            name_57bytes = None
            size = None
            md5 = None
            # FIXME
            def __str__(self):
                return "str(dict(self))"
        # parse filenames and MD5 hashes from the nfo file
        integrity = IntegrityData()
        subs_format = None
        subs_format_entries = []
        for line in nfo_file_content.split("\n"):
            # note: positions are measured in utf8 characters
            # line[0:1] == "Û"
            # line[0:1].encode("utf8") == b'\xc3\x9b'
            # bug in the opensubtitles nfo generator:
            # when the filename contains utf8 chars,
            # then its length is counted in bytes
            # when it should be counted in utf8 chars
            # so, when there is a two-byte utf8 char in the name,
            # the filename is cut 1 character too early
            # this can produce invalid utf8 chars
            # when the cut removes bytes from a utf8 char
            # example:
            # "ö".encode("utf8") == b'\xc3\xb6'
            # b'\xc3\xb6'[0:1].decode("utf8")
            # -> UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data
            if line.startswith("Û  Filename       : "):
                integrity.name_57bytes = line[20:-2].rstrip().rstrip("\x00")
            elif line.startswith("Û  Size           : "):
                size_str = line[20:-2].rstrip() # "12345 bytes"
                assert size_str.endswith(" bytes")
                size_str = size_str[0:-6]
                integrity.size = int(size_str)
            elif line.startswith("Û  MD5            : "):
                integrity.md5 = line[20:-2].rstrip().lower()
                assert len(integrity.md5) == 32
                zip_file_integrity_data.append(integrity)
                # next file
                integrity = IntegrityData()
            elif line.startswith("Û  Format         : "):
                this_subs_format = line[20:-2].rstrip().lower()
                subs_format_entries.append(this_subs_format)
                if subs_format == None:
                    subs_format = this_subs_format
        integrity = None
        if len(subs_format_entries) > 1:
            print(f"sub {group_name} {sub_number}: nfo file has multiple Format entires:", subs_format_entries)
        del subs_format_entries

        # column: SubFormat
        if subs_format != sub_metadata_subs_format:
            print(f"sub {group_name} {sub_number}: subs_format was changed from nfo to metadata: from {subs_format} to {sub_metadata_subs_format}")
            # prefer subs_format from opensubs-metadata.db
            subs_format = sub_metadata_subs_format

        #print("zip_file_integrity_data", zip_file_integrity_data) # debug

        for zip_info in sub_zip.infolist():
            # this fails when filename is empty
            # File "python3-3.10.9/lib/python3.10/zipfile.py", line 530, in is_dir
            # return self.filename[-1] == '/'
            #if zip_info.is_dir():
            # fixed in new version of python
            if zip_info.filename.endswith("/"):
                continue
            # Access a member of the archive as a binary file-like object
            # The mode parameter, if included, must be 'r' (the default) or 'w'.
            with sub_zip.open(zip_info, "r") as src:
                # TODO? rename to file_content_bytes
                file_content = src.read()
            filepath = zip_info.filename

            if filepath.endswith(f".({sub_number}).nfo"):
                # remove the opensubtitles nfo files
                # all useful data (FPS, IMDB) is also stored in subtitles_all.txt
                # the nfo files also contain sizes and MD5 hashes of the subtitle files
                # these sizes and hashes can be useful to verify the orignal files
                # but we recode the files to utf8, so the hashes are different
                continue

            # é -> latin1 hex e9, dec 233
            # La cité des enfants perdus
            # sub 199799: weird filename '[58237] Cit.233; des enfants perdus, La (1995).srt'
            # sub 3305602: weird filename 'La cit├⌐ des enfants perdus.srt'
            # sub 3526667: weird filename 'La cit├⌐ des enfants perdus (The City of Lost Children) .srt'
            # https://stackoverflow.com/a/29351191/10440128
            # $ echo -n '├⌐' | hexdump -C
            # e2 94 9c e2 8c 90
            # $ echo -n 'é' | hexdump -C
            # c3 a9
            # $ printf "\xc3\xa9" | iconv -f cp437 -t utf8
            # ├⌐
            # $ printf 'é' | iconv -f cp437 -t utf8
            # ├⌐
            # $ printf '├⌐' | iconv -f utf8 -t cp437
            # é
            # https://stackoverflow.com/questions/13261347/correctly-decoding-zip-entry-file-names-cp437-utf-8-or
            # TODO allow more chars?
            # TODO use import charset_normalizer
            filepath_encoding_id_of_encoding = {
                "ascii": 1,
                "utf8": 2,
                "cp437": 3,
                "iso-8859-1": 4,
                "cp1254" : 5, # aka "Windows-1254" # some czech subs
                # TODO more?
            }
            filepath_encoding = "ascii"
            if not re.fullmatch(r"""[a-zA-Z0-9 \[\](){}.,;:_?!'"~+-]*""", filepath):
                # filepath is not ascii text
                done_fix = False
                for encoding in ["cp437", "iso-8859-1"]:
                    try:
                        # '├⌐'.encode("cp437").decode("utf8") == 'é'
                        fixed_filepath = filepath.encode(encoding).decode("utf8")
                        filepath_encoding = encoding
                    except UnicodeEncodeError:
                        continue
                    if filepath_encoding != "cp437":
                        print(f"sub {group_name} {sub_number}: recoding file name from {filepath_encoding} to utf8: from {repr(filepath)} to {repr(fixed_filepath)}")
                    filepath = fixed_filepath
                    done_fix = True
                    break
                if not done_fix:
                    print(f"sub {group_name} {sub_number}: keeping weird file name {repr(filepath)}")

            # verify file name and content
            if False:
                filepath_57bytes = None
                try:
                    filepath_57bytes = filepath.encode("utf8")[0:57].decode("utf8")
                except UnicodeDecodeError as err:
                    print(
                        f"sub {sub_number}: found rare edge-case: "
                        f"cutting the file name {repr(filepath)} after 57 bytes produces invalid utf8: {err}. "
                        f"this means we currently cannot validate the file name, "
                        f"and probably we cannot read the nfo file"
                    )
            # filepath_57bytes is not reliable to find all files
            # so we find files by size and hash
            done_file_integrity = False
            file_content_md5 = hashlib.md5(file_content).hexdigest()
            file_content_len = len(file_content)
            for integrity in zip_file_integrity_data:
                if not (
                    file_content_len == integrity.size and
                    file_content_md5 == integrity.md5
                ):
                    continue
                #if filepath_57bytes and not filepath_57bytes == integrity.name_57bytes:
                # integrity.name_57bytes can be shorter because rstrip("\x00"), so compare prefix
                if not filepath.startswith(integrity.name_57bytes):
                    # sub 3238015: failed to verify file name 'The Legend of Drunken Heb.srt'. actual filepath 'The Legend of Drunken Heb.srt'. expected prefix 'The Legend Of Drunken Master.srt'
                    # "Heb" means the "Hebrew" language
                    print(
                        f"sub {sub_number}: info: failed to verify file name {repr(filepath)}. "
                        f"actual filepath {repr(filepath)}. "
                        f"expected prefix {repr(integrity.name_57bytes)}"
                    )
                done_file_integrity = True
                break
            if not done_file_integrity:
                print(
                    f"sub {sub_number}: error: failed to verify file content {repr(filepath)}. "
                    f"actual size {file_content_len}. "
                    f"actual md5 {file_content_md5}"
                )

            # cleanup filename
            # TODO be more conservative and keep filenames?
            # usually we dont care at all about filenames
            # its more important to have correct file extensions
            # https://www.ascii-code.com/
            filepath = filepath.replace("\xa0", " ") # \xa0 = &nbsp; = Non-breaking space
            # https://en.wikipedia.org/wiki/Quotation_mark#Unicode_code_point_table
            # replace fancy single quotes with apostrophe = &apos; = straight single quote
            filepath = filepath.replace("´", "'") # acute accent
            filepath = filepath.replace("‘", "'") # left single quotation mark
            filepath = filepath.replace("’", "'") # right single quotation mark
            filepath = filepath.replace("‚", "'") # single low-9 quotation mark
            filepath = filepath.replace("‛", "'") # single high-reversed-9 quotation mark
            filepath = filepath.replace("＇", "'") # fullwidth apostrophe
            # replace fancy double quotes with quotation mark = &quot; = straight double quote
            filepath = filepath.replace("“", '"') # left double quotation mark
            filepath = filepath.replace("”", '"') # right double quotation mark
            filepath = filepath.replace("„", '"') # double low-9 quotation mark
            filepath = filepath.replace("‟", '"') # double high-reversed-9 quotation mark
            filepath = filepath.replace("⹂", '"') # double low-reversed-9 quotation mark
            filepath = filepath.replace("＂", '"') # fullwidth quotation mark

            # fix empty file name
            #print(f"sub {group_name} {sub_number}: zip_info.filename:", repr(zip_info.filename))
            if filepath == "":
                # example: 237742.kamikaze.girls.(2004).eng.1cd
                # the file with empty filename is a SAMI file
                # https://en.wikipedia.org/wiki/SAMI
                # magic_result.mime_type = "text/html"
                # TODO also use SubFormat from subtitles_all/subtitles_all.txt.gz-parse-result.txt
                # here:
                # {"IDSubtitle": 237742, "MovieName": "Kamikaze Girls", "MovieYear": 2004, "LanguageName": "English", "ISO639": "en", "SubAddDate": "2006-03-23 00:00:00", "ImdbID": 416220, "SubFormat": "smi", "SubSumCD": 1, "MovieReleaseName": "Shimotsuma Monogatari (2004)", "MovieFPS": 0.0, "SeriesSeason": 0, "SeriesEpisode": 0, "SeriesIMDBParent": 0, "MovieKind": "movie", "URL": "http://www.opensubtitles.org/subtitles/237742/kamikaze-girls-en"}
                # "SubFormat": "smi"
                # "smi" is short for "sami"
                # "smi" file extension is ambiguous, so "sami" is better
                print(f"sub {group_name} {sub_number}: empty filename in zip_path {repr(zip_path)}. using zip_basename {repr(zip_basename)}")
                if False:
                    print(f"sub {group_name} {sub_number}: zip_path", repr(zip_path))
                    print(f"sub {group_name} {sub_number}: zip_name", repr(zip_name))
                    print(f"sub {group_name} {sub_number}: zip_basename", repr(zip_basename))
                    raise NotImplementedError
                filepath = zip_basename
                # TODO use the sub language as a hint?
                # TODO add the guessed encoding to the filename
                # example: Scary.Movie.latin1_utf8.srt
                # ... if the sub was converted from latin1 to utf8
                # or maybe just
                # example: Scary.Movie.latin1_.srt
                # because all files are utf8
                # TODO verify: also encode utf16/utf32 to utf8 and remove byte-order-marks (BOM)
                # simple: use subs_format as file extension
                filepath += "." + subs_format
                """
                magic_result = magic.detect_from_content(file_content)
                print(f"sub {group_name} {sub_number}: empty filename in zip_path {repr(zip_path)}. magic_result {repr(magic_result)}")
                if magic_result.mime_type.startswith("text/html"):
                    if re.search(b"<sami>", file_content, re.I):
                        filepath += ".sami"
                    else:
                        filepath += ".html"
                elif magic_result.mime_type.startswith("text/"):
                    if is_sub_content(file_content):
                        filepath += ".sub"
                    else:
                        filepath += ".txt"
                else:
                    #filepath += ".unknown"
                    print(f"sub {group_name} {sub_number}: empty filename in zip_path {repr(zip_path)}. magic_result {repr(magic_result)} TODO add file extension")
                    raise NotImplementedError(f"TODO add file extension to sub file in sub {group_name} {sub_number}")
                """
                print(f"sub {group_name} {sub_number}: empty filename in zip_path {repr(zip_path)}. using filepath {repr(filepath)}")
            # fix wrong file extension
            elif filepath[-4:].lower() == ".txt":
                if is_sub_content(file_content):
                    filepath = filepath[0:-4] + ".sub"
                    # verbose
                    #print(f"sub {group_name} {sub_number}: renaming txt file to sub file {repr(filepath)}")
                    # TODO also print subs_format. is it "sub"? then use format as extension

            # convert file path to utf8
            if convert_to_utf8:
                #if metadata_encoding not in {"us-ascii", "utf-8", "unknown-8bit", "binary"}: # magic
                if metadata_encoding not in {"ascii", "utf_8"}: # charset_normalizer
                    # TODO test
                    print(f"sub {group_name} {sub_number}: converting file path from {metadata_encoding} to utf8: {repr(filepath)}")
                    # TODO verify. usually type(filepath) == str
                    #filepath = filepath.decode(metadata_encoding).encode("utf8") # bytes -> str -> bytes
                    # zipfile returns all filenames as utf8 strings
                    filepath = filepath.encode("utf8").decode(metadata_encoding) # str -> bytes -> bytes

            # no. keep all original file extensions
            # only add a new extension if necessary
            """
            # example: sub 110892: Alien 3.srt.txt
            if filepath.endswith(f".{subs_format}.txt"):
                filepath = filepath[:-4]

            # assume that the original zip files dont contain zip files
            # example: sub imdb-379217-len-20 90562: adding sub file extension to file name '161118.zip' -> '161118.zip.sub'
            # the original zipfile 000090562.coffee.and.cigarettes.(2003).cze.1cd.(90562).zip
            # really contains a subtitles file called "161118.zip" but its a *.sub file
            if filepath.endswith(".zip"):
                filepath = filepath[:-4]
            """

            # based on subtitles_all.txt.gz-parse.py
            expr_filename_extension = r"\.(srt|sub|txt|mpl|smi|ssa|ass|tmp|vtt)"

            # fix file extension
            filepath_base, filename_extension = os.path.splitext(filepath)
            # simple: use subs_format as file extension
            if filename_extension != "." + subs_format:
                filepath_new = None
                """
                if re.fullmatch(expr_filename_extension, filename_extension.lower()):
                    # remove old extension
                    filepath_new = filepath_base + "." + subs_format
                else:
                    # keep old extension
                    filepath_new = filepath + "." + subs_format
                """
                # keep old extension, append new extension
                filepath_new = filepath + "." + subs_format
                print(f"sub {group_name} {sub_number}: adding sub file extension to file name {repr(filepath)} -> {repr(filepath_new)}")
                filepath = filepath_new

            # limit filename length to 255 bytes
            filepath = cut_filename(filepath)

            """
            if not re.fullmatch(expr_filename_extension, filename_extension.lower()):
                if is_sub_content(file_content):
                    filepath += ".sub"
                    #print(f"sub {group_name} {sub_number}: adding sub file extension to file name {repr(filepath)}")
                # TODO more
                else:
                    print(f"sub {group_name} {sub_number}: FIXME no file extension in file name {repr(filepath)}")
                    # example: sub 62421: 'Magnolia (DVDRip - DivX5.02 - dual English . Espa'
                    # ". Espa" is not a file extension
                    # sub 193348: FIXME no file extension in file name 'Bez ko'
                    # sub 2364: FIXME no file extension in file name '[1965] Luis Bunuel - Sim'
                    # sub 3458687: FIXME no file extension in file name 'Le Charme Discret de la Bourgeoisie [Discreet Charm of the Bourgeoisie] (1972, Buńuel) [DVDivX-SC.sr'
                    # sub 2371: FIXME no file extension in file name '[1974] Luis Bunuel - Le Fant'
                    # sub 2373: FIXME no file extension in file name '[1977] Luis Bunuel - Cet obscur objet du d'
                    # sub 89545: FIXME no file extension in file name 'La.'
                    # sub 4019866: FIXME no file extension in file name '1993 - Martin Scorsese - The age of innocence - Con Daniel Day Lewis, Michelle Pfeiffer, Winona Ryde'
                    # sub 3086906: FIXME no file extension in file name 'Antonioni, Michelangelo . Wenders, Wim - Al Di Là Delle Nuvole - DivX 3 x 512 x 304 . 23,976 - Audio'
                    # sub 2432: FIXME no file extension in file name '[1967] Milos Forman - Ho'
                    # sub 3110428: FIXME no file extension in file name 'Mikhalkov, Nikita - (1979) - Neskolko dnej iz zhizni I.I. Oblomova - CD 2 - XviD - 540 x 416 . 25 -'
                    # sub 3557338: FIXME no file extension in file name '[HDteam] Austin.Powers.La.Espia.que.me.Achucho.1999.[1080p].[BluRay].[x264].[Dual].[Subs]-by Ruben.E'
                    # sub 114261: FIXME no file extension in file name 'contact_eng'
                    # sub 3103577: FIXME no file extension in file name 'Kubrick, Stanley - (1960) - Spartacus - CD 1 - DivX 5 - 576 x 240 . 25 - English - Separate Subtitle'
                    # sub 4089080: FIXME no file extension in file name 'srt'
                    # sub 3377354: FIXME no file extension in file name "The Bishop's Wife (1947) Henry Koster (Cary Grant Loretta Young David Niven) Gregg Toland6 engish.sr"
                    # sub 3321622: FIXME no file extension in file name 'Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl.Remastered.2003.720p.BluRay.DTSx264-ESiR.eng.s'
                    # sub 136869: FIXME no file extension in file name 'Pirates of the cara'
                    # sub imdb-112682-len-193 35495: FIXME no file extension in file name 'La cit'
                    # maybe the filename is truncated after the first "illeagl" character
                    # probably 'La cit' is the prefix of 'La cité des enfants perdus'
            """

            # rename duplicate files
            done_rename = False
            old_filepath = filepath
            old_filepath_base, filename_extension = os.path.splitext(filepath)
            if not re.fullmatch(expr_filename_extension, filename_extension.lower()):
                old_filepath_base, filename_extension = filepath, ""
            duplicate_number = 1
            this_filename_filepaths = [filepath]
            while filepath in seen_filepaths:
                # TODO check for duplicate file_content
                # if file exists with exact same file_content, then keep only the first file
                # files can differ only in encoding
                # for example one file in utf8, other file in latin1
                # file numbers are one-based:
                # 1 = original file
                # 2 = duplicate 1
                # 3 = duplicate 2
                duplicate_number += 1
                filepath = f"{old_filepath_base}.{duplicate_number}{filename_extension}"
                this_filename_filepaths.append(filepath)
                done_rename = True

            #print(f"sub {group_name} {sub_number}: filepath", filepath)
            # extract cannot change the filename
            #sub_zip.extract(zip_info, path=filepath) # pwd=sub_tempdir.encode("utf8") # has no effect?
            filepath_dirname = os.path.dirname(filepath)
            if filepath_dirname != "":
                os.makedirs(filepath_dirname, exist_ok=True)

            # TODO repack old groups with old filepath_prefix format
            # see logfile: new format starts at
            #   group the.amityville.horror.1979.eng
            #   sub 50324: sub_num_parts '2cd'
            filepath_prefix = f"{sub_number}."
            # no, too complex. multi-part subs will have multiple files, which is self-explaining
            # also opensubs-metadata.db stores number of parts (1cd, 2cd, 3cd, ...)
            """
            if sub_num_parts != "1cd":
                #print(f"sub {group_name} {sub_number}: sub_num_parts {repr(sub_num_parts)}")
                filepath_prefix = f"{sub_number}-{sub_num_parts}."
            """

            # dont use sub_tempdir
            # 1cd-subs (98% of all subs) contain only one file
            # having only one file per folder is a waste of inodes
            # assuming that on extraction, the archive's file layout is used
            # so we put all files in group_tempdir
            # and we prefix all files with sub_number
            #with open(f"{sub_tempdir}/{filepath}", "wb") as dst:
            file_content_recoded_from = None
            filepath_abs = f"{group_tempdir}/{filepath_prefix}{filepath}"
            with open(filepath_abs, "wb") as dst:
                if convert_to_utf8:
                    # FIXME broken output: 5362013.Alien 3.sub

                    #content_encoding = magic.detect_from_content(file_content).encoding
                    # use charset_normalizer
                    metadata_charset_matches = charset_normalizer.from_bytes(file_content)
                    metadata_charset_best_match = metadata_charset_matches.best()
                    content_encoding = metadata_charset_best_match.encoding

                    """
                    #if content_encoding != "ascii" or metadata_charset_best_match.language != lang_code_charset_normalizer:
                    if metadata_charset_best_match.language != lang_code_charset_normalizer:
                        print("filepath:", filepath_abs)
                        print("file_content: file_content_bytes[1000:1200]", repr(file_content[1000:1200]))
                        print("file_content: metadata_charset_matches", metadata_charset_matches)
                        #print("file_content: metadata_charset_best_match", metadata_charset_best_match)
                        print("file_content: metadata_charset_best_match.could_be_from_charset", metadata_charset_best_match.could_be_from_charset)
                        print("file_content: metadata_charset_best_match.encoding", metadata_charset_best_match.encoding)
                        print("file_content: metadata_charset_best_match.language", metadata_charset_best_match.language)
                        print("file_content: metadata_charset_best_match.languages", metadata_charset_best_match.languages)
                    """

                    #if content_encoding not in {"us-ascii", "utf-8", "unknown-8bit", "binary"}: # magic
                    if content_encoding not in {"ascii", "utf_8"}: # charset_normalizer
                        # convert file file_content to utf8
                        # TODO test
                        if verbose_recode_content:
                            print(f"sub {group_name} {sub_number}: recoding file content from {content_encoding} to utf8: {repr(filepath)}")
                        file_content = file_content.decode(content_encoding).encode("utf8") # bytes -> str -> bytes
                        file_content_recoded_from = content_encoding
                dst.write(file_content)

            if file_content_recoded_from:
                filepath_base, filename_extension = os.path.splitext(filepath)
                filepath = f"{filepath_base}.{file_content_recoded_from}_utf8{filename_extension}"

            if done_rename:
                # FIXME? sub 62421: renaming duplicate file from 'Magnolia (DVDRip - DivX5.02 - dual English . Espa' to 'Magnolia (DVDRip - DivX5.02 - dual English .2. Espa'
                # FIXME check for duplicate file content via size and sha1
                this_filename_filepaths.pop() # last value is filepath
                this_filepath_size = os.path.getsize(filepath)
                this_filepath_hash = sha1sum(filepath)
                #print(f"  {this_filepath_hash}  {filepath}")
                duplicate_filepath = None
                for other_filepath in this_filename_filepaths:
                    other_filepath_size = os.path.getsize(other_filepath)
                    if other_filepath_size != this_filepath_size:
                        # content is different
                        continue
                    other_filepath_hash = sha1sum(other_filepath)
                    #print(f"  {other_filepath_hash}  {other_filepath}")
                    if other_filepath_hash != this_filepath_hash:
                        # content is different
                        continue
                    # content is equal -> remove this filepath
                    duplicate_filepath = other_filepath
                    break # stop looking for duplicates
                if duplicate_filepath:
                    print(f"sub {group_name} {sub_number}: ignoring duplicate file {repr(old_filepath)} with same content as {repr(duplicate_filepath)}")
                    os.unlink(filepath)
                else:
                    seen_filepaths.add(filepath)
                    print(f"sub {group_name} {sub_number}: renaming duplicate file from {repr(old_filepath)} to {repr(filepath)}")



# based on https://stackoverflow.com/a/75165440/10440128
def cut_filename(filename):
    # limit filename length to 255 bytes
    max_filename_length = 255
    filename_bytes = filename.encode("utf8")
    if len(filename_bytes) > max_filename_length:
        cut_bytes = filename_bytes[:max_filename_length]
        filename_new_size = len(cut_bytes.decode("utf8", errors="ignore"))
        cut_idx = int(filename_new_size / 2) - 1
        filename = filename[:cut_idx] + "..." + filename[(-1 * cut_idx):]
    return filename



main()
