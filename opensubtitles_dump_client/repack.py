#! /usr/bin/env python3

# python -u ./repack.py | tee -a repack.py.log

# good signs:
# - no "keeping weird file name"
# - no "failed to verify file" (except nfo files, but we dont have checksums for nfo files)

import os
import json
import subprocess
import glob
import sqlite3
import zipfile
import shutil
import tarfile
import lzma
import re
import hashlib

import magic # libmagic


lang_code = "eng"

# generated by index.py and index-compress.py
# TODO merge index-compress.py with index.py
index_file = f"index-grouped/index.txt.grouped.{lang_code}"

metadata_db_path = "../subtitles_all/subtitles_all.txt.gz-parse-result.db"

opensubs_db_path = "opensubs.db"

tempdir = f"/run/user/{os.getuid()}/opensubs-repack" # tmpfs

output_dir = f"repack-grouped/{lang_code}"

cached_zip_dir = "../done-zips"

convert_to_utf8 = False
convert_to_utf8 = True
verbose_recode_content = False

# txt files are not recognized as subtitles
# fix: rename to sub files
# https://github.com/mpv-player/mpv/issues/4144
rename_txt_to_sub = True

def is_sub_content(file_content):
    # example sub file:
    #   {1}{1}25.000
    #   {123}{234}first line|second line
    #   {345}{456}just one line
    #   {112269}{113769}...
    # the FPS line "{1}{1}25.000" is optional
    for line in file_content.split(b"\n"):
        # \s*: allow lines with only whitespace
        if not re.fullmatch(rb"\{\d+\}\{\d+\}.*|\s*", line):
            return False
    return True

debug_stop_after_first_group = False
#debug_stop_after_first_group = True

debug_stop_after_n_groups = None
#debug_stop_after_n_groups = 1000

# good for real-world users:
# have one movie, get all subs in one language
# bad for updates: old archives should be immutable
repack_strategy = "group-by-movie-name"

# actually this requires a first pass with group-by-movie-name
# and in the second pass, we can group-by-movie-name-hash
#repack_strategy = "group-by-movie-name-hash"
repack_strategy_hash_bits = 16 # hash size in bits

# bad for real-world users:
# subs for one movie are fragmented
# across many archives
# good for updates: old archives are immutable
#repack_strategy = "group-by-sub-number"
repack_strategy_group_size = 100
# bad compression: only 3% better than zip files, see readme.md


# global state
index_data = None
metadata_db_connection = None
metadata_db_cursor = None
cached_zip_files = None
opensubs_db_connection = None
opensubs_db_cursor = None


def main():
    global index_file
    global index_data
    global metadata_db_path
    global metadata_db_connection
    global metadata_db_cursor
    global cached_zip_files
    global opensubs_db_connection
    global opensubs_db_cursor
    global lang_code

    os.makedirs(output_dir, exist_ok=True)

    with open(index_file, "r") as f:
        index_data = json.load(f)

    metadata_db_connection = sqlite3.connect(metadata_db_path)
    # default: rows are tuples (faster)
    #metadata_db_connection.row_factory = sqlite3.Row # rows are dicts
    metadata_db_cursor = metadata_db_connection.cursor()
    # metadata_data

    cached_zip_files = glob.glob(f"{cached_zip_dir}/*.zip")
    cached_zip_files_by_num = dict()
    for zip_file in cached_zip_files:
        num = int(os.path.basename(zip_file).split(".", 2)[0])
        cached_zip_files_by_num[num] = zip_file
    cached_zip_files = cached_zip_files_by_num
    #print("cached zip files:", len(cached_zip_files))

    if repack_strategy == "group-by-movie-name":
        done_groups = 0
        for movie_name_year in index_data:
            repacked_movie = repack_by_movie_name(movie_name_year, lang_code)
            if repacked_movie == False:
                # dont count
                continue
            done_groups += 1
            if debug_stop_after_first_group:
                break
            if debug_stop_after_n_groups and done_groups >= debug_stop_after_n_groups:
                break
            print()

    elif repack_strategy == "group-by-sub-number":
        group_first_sub_number = 1
        last_sub_number = 9180517 # opensubs.db-last-num.sh
        archive_number = 0
        index_file = open("index.txt", "r")
        while group_first_sub_number < last_sub_number:
            archive_number += 1
            # avoid reading sqlite - prefer index.txt
            #opensubs_db_cursor.execute(
            #    "select num from subz where num >= (?) limit (?)",
            #    (group_first_sub_number, repack_strategy_group_size)
            #)
            #sub_numbers = opensubs_db_cursor.fetchall()
            sub_numbers = []
            end_of_input = False
            while len(sub_numbers) < repack_strategy_group_size:
                try:
                    line = index_file.readline()
                # TODO be more specific
                except Exception as err:
                    print("end of index_file?", err)
                    end_of_input = True
                    break
                sub_number = int(line.split(" ", 2)[0])
                sub_numbers.append(sub_number)
            print(f"archive {archive_number}: sub_numbers:", sub_numbers)
            repack_by_sub_numbers(sub_numbers)
            group_first_sub_number = sub_numbers[-1] + 1
            if debug_stop_after_first_group:
                break
            if end_of_input:
                break

    elif repack_strategy == "group-by-movie-name-hash":
        #repack_strategy_hash_bits = 16 # hash size in bits
        raise NotImplementedError


def repack_by_sub_numbers(sub_numbers):
    range_str = f"{sub_numbers[0]}-{sub_numbers[-1]}"
    tarxz_path = f"{output_dir}/{range_str}.tar.bz2"
    if os.access(tarxz_path, os.R_OK):
        return

    group_tempdir = f"{tempdir}/{range_str}"
    shutil.rmtree(group_tempdir, ignore_errors=True)
    os.makedirs(group_tempdir) # exist_ok=True
    print("group_tempdir", group_tempdir)

    print("sub_numbers[0]", sub_numbers[0])


def filter_tarinfo(tarinfo):
    # reset file owner to zero
    # TODO reset file time
    # TODO numeric owner
    tarinfo.uid = tarinfo.gid = 0
    tarinfo.uname = tarinfo.gname = "root"
    return tarinfo


def repack_by_movie_name(movie_name_year, lang_code):
    # remove parens from movie year
    movie_name_year_original = movie_name_year.replace(" ", ".") # alien.3.(1992)
    movie_name_parts = movie_name_year.split(" ") # ["alien", "3", "(1992)"]
    movie_year = movie_name_parts.pop() # "(1992)"
    movie_name = " ".join(movie_name_parts) # "alien 3"
    assert movie_year.startswith("(") and movie_year.endswith(")")
    movie_year = movie_year[1:-1] # "1992"
    if movie_year == "":
        movie_year = "0"
    else:
        assert movie_year == str(int(movie_year))

    if False:
        print(f"movie_name_year", movie_name_year)
        print(f"movie_name", movie_name)
        print(f"movie_year", movie_year)
        raise NotImplementedError

    if movie_name == "":
        raise NotImplementedError("empty movie name")

    group_name = f"""{movie_name.replace(" ", ".")}.{movie_year}.{lang_code}""" # alien.3.1992.eng
    # TODO remove parens from group_name?
    # TODO remove multiple dots from group_name? re.replace(r"\.{2,}", ".", group_name)
    # dir camel(s).2001.eng: ()
    # dir va.savoir.(who.knows).2001.eng: ()
    # dir on_line.2002.eng: _
    assert (
        re.fullmatch(r"[a-z0-9.()_]+\.\d{1,4}\." + lang_code, group_name)
    ), (
        f"invalid group name: {group_name}"
    )

    group_tempdir = f"{tempdir}/{group_name}"
    tar_xz_path = f"{output_dir}/{group_name}.tar.xz"
    if os.access(tar_xz_path, os.R_OK):
        #print(f"exists: {tar_xz_path}")
        return False
    movie_subs = index_data[movie_name_year]

    group_tempdir = f"{tempdir}/{group_name}"
    shutil.rmtree(group_tempdir, ignore_errors=True)
    os.makedirs(group_tempdir) # exist_ok=True
    #print("group_tempdir", group_tempdir)

    #print("movie_subs[0]", movie_subs[0])
    print(f"dir {group_name}")

    # TODO advanced duplicate detection
    # when two subs have the exact same text
    # only with a small offset (like 1 second)
    # then keep only one sub (the more popular one)
    # subtitle sync can be automated with tools like ffsync
    # but probably this gives only about 5% smaller size of archive
    # so its not worth the complexity
    num_subs = 0
    for sub_number, _sub_numparts, _sub_positions, _crc16 in movie_subs:
        metadata_db_cursor.execute("select MovieKind from subz_metadata where IDSubtitle = ?", (sub_number,))
        result = metadata_db_cursor.fetchone()
        if result == None:
            # sub 9180120
            # $ grep -m1 ^9180120 ../subtitles_all/subtitles_all.txt
            # $ sqlite3 -header ../subtitles_all/subtitles_all.txt.gz-parse-result.db "select * from subz_metadata where IDSubtitle = 9180120"
            print(f"sub {group_name} {sub_number}: FIXME missing metadata, cannot get MovieKind, skipping this sub")
            #raise NotImplementedError("missing metadata")
            continue
        (sub_MovieKind,) = result
        #print(f"sub {group_name} {sub_number}: sub_MovieKind {sub_MovieKind}")
        if sub_MovieKind == "tv":
            print(f"sub {group_name} {sub_number}: TODO implement MovieKind=tv")
            # TODO reduce number of archives by grouping seasons of TV shows
            # inside the archive, create one folder per episode
            # use strict format for season numbers and episode numbers
            # s01.e01
            # s01e01
            # season1.episode1
            continue
        extract_movie_sub(sub_number, group_tempdir, movie_name_year_original, movie_name, movie_year)
        num_subs += 1

    if num_subs == 0:
        print(f"dir {group_name}: no subs")
        return

    #print("group_tempdir", group_tempdir)

    # TODO create reproducible archives with python tarfile
    #compress_group_tempdir_in_memory(group_tempdir, tar_xz_path)
    #compress_group_in_file(group_tempdir, tar_xz_path)
    compress_group_in_file_shell(group_tempdir, tar_xz_path)

    content_size = path_getsize_recursive(group_tempdir)
    tar_xz_size = os.path.getsize(tar_xz_path)
    zip_files_size = 0
    for sub_number, _sub_numparts, _sub_positions, _crc16 in movie_subs:
        # sub 9175277: FIXME missing metadata, cannot get MovieKind, skipping this sub
        # KeyError: 9175277
        # TODO why is sub_number in movie_subs but not in metadata
        # its not in cached_zip_files because we did not process this sub
        if not sub_number in cached_zip_files:
            continue
        zip_file = cached_zip_files[sub_number]
        zip_files_size += os.path.getsize(zip_file)

    smaller_percent = (zip_files_size - tar_xz_size) / zip_files_size * 100

    # print compression stats
    tar_xz_name = os.path.basename(tar_xz_path)
    print(f"tar {tar_xz_name}: raw {content_size / 1000:.0f}K zips {zip_files_size / 1000:.0f}K xz {tar_xz_size / 1000:.0f}K = {smaller_percent:.0f}% smaller")

    # cleanup
    shutil.rmtree(group_tempdir, ignore_errors=True)


# https://stackoverflow.com/a/1392549/10440128
def path_getsize_recursive(start_path = '.'):
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(start_path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            # skip if it is symbolic link
            if not os.path.islink(fp):
                total_size += os.path.getsize(fp)

    return total_size


def compress_group_tempdir_in_memory(group_tempdir, tar_xz_path):
    """
    create tar.xz file in memory
    """
    global tempdir
    # TODO sort files to get reproducible archive
    # https://reproducible-builds.org/docs/archives/
    #tar_xz_path = f"{tempdir}/{group_name}.py.tar.xz"
    #print(f"writing {tar_xz_path}")
    with tarfile.open(tar_xz_path, "w:xz") as tar:
        # TODO sort files to get reproducible archive
        for sub_dir in os.listdir(group_tempdir):
            if os.path.isfile(sub_dir):
                continue
            if sub_dir == "split-archives": continue # TODO remove
            tar.add(f"{group_tempdir}/{sub_dir}", sub_dir, filter=filter_tarinfo)
    #print(f"writing {tar_xz_path} done")

    # 453120  expected size
    # 467912  /run/user/1000/alien 3 (1992).tar.xz # convert_to_utf8=True
    # 467248  /run/user/1000/alien 3 (1992).tar.xz # convert_to_utf8=False
    # blame utf8?


def compress_group_in_file(group_tempdir, tar_xz_path):
    """
    create tar.xz file from tar file
    """
    tar_path = tar_xz_path[0:-3] # remove ".xz"
    print(f"writing {tar_path} ...")
    # format=tarfile.GNU_FORMAT gives smaller files
    # https://stackoverflow.com/questions/73643142/large-size-of-the-tar-files-generated-by-python-tarfile
    with tarfile.open(tar_path, "w", format=tarfile.GNU_FORMAT) as tar:
        # TODO sort files to get reproducible archive
        # https://reproducible-builds.org/docs/archives/
        for sub_dir in os.listdir(group_tempdir):
            if os.path.isfile(sub_dir):
                continue
            if sub_dir == "split-archives": continue # TODO remove
            tar.add(f"{group_tempdir}/{sub_dir}", sub_dir, filter=filter_tarinfo)
    print(f"writing {tar_path} done")
    print(f"writing {tar_xz_path} ...")
    with (
        open(tar_path, "rb") as src,
        lzma.open(tar_xz_path, "w", preset=6) as dst
    ):
        dst.write(src.read())
    print(f"writing {tar_xz_path} done")


def compress_group_in_file_shell(group_tempdir, tar_xz_path):
    """
    create tar.xz file from tar file
    using the external tar program
    """
    tar_path = tar_xz_path[0:-3] # remove ".xz"
    group_name = os.path.basename(group_tempdir)
    #print(f"writing {tar_xz_path} ...")
    tar_files = []
    tar_cwd = None
    if False:
        # store multiple folders in the archive
        for sub_dir in os.listdir(group_tempdir):
            if os.path.isfile(sub_dir):
                continue
            if sub_dir == "split-archives": continue # TODO remove
            tar_files.append(sub_dir)
        # tar_files are relative to group_tempdir
        tar_cwd = group_tempdir
    else:
        # store one folder in the archive
        tar_files = [os.path.basename(group_tempdir)]
        # tar_files are relative to parent dir of group_tempdir
        tar_cwd = os.path.dirname(group_tempdir)
    # need absolute path because we change cwd
    tar_xz_path_abs = os.path.abspath(tar_xz_path)
    subprocess.run(
        [
            "tar",
            # all these options are required to create reproducible archives
            # https://reproducible-builds.org/docs/archives/
            # TODO create reproducible archives with python tarfile
            # so this also works on windows
            "--format=gnu",
            "--sort=name", # sort filenames, independent of locale. tar v1.28
            "--mtime=0",
            "--owner=0",
            "--group=0",
            "--numeric-owner",
            "-c", # create archive
            "--xz", # compress with xz (default settings: level 6, no --extreme)
            "-f", tar_xz_path_abs, # output file
        ] + tar_files, # input files
        cwd=tar_cwd,
        check=True,
    )
    #print(f"writing {tar_xz_path} done")

    if False:
        # run xz separately to compress .tar to .tar.xz
        # this gives the exact same result as "tar --xz"
        print(f"writing {tar_xz_path} ...")
        subprocess.run(
            [
                "xz",
                #"-k", # keep input file
                tar_path,
            ],
            check=True,
        )
        print(f"movie {group_name}: writing {tar_xz_path} done")


def extract_movie_sub(sub_number, group_tempdir, movie_name_year_original, movie_name, movie_year):
    # TODO split this function
    # TODO recode in a separate function (convert_to_utf8)
    # TODO test recode
    # TODO validate zip archive? (how? no checksums)
    # TODO validate extracted files? (CRC32 in zip metadata)
    # TODO remove parens from group_name

    global cached_zip_files
    global cached_zip_dir
    global opensubs_db_cursor

    group_name = os.path.basename(group_tempdir)

    # get the zip file
    if sub_number in cached_zip_files:
        zip_path = cached_zip_files[sub_number] # example: ../done-zips/000000123.pulp.fiction.zip
        zip_name = os.path.basename(zip_path).split(".", 1)[1] # example: pulp.fiction.zip
        zip_basename = zip_name[0:-4] # example: pulp.fiction
    else:
        if not opensubs_db_cursor:
            # connect only on demand
            opensubs_db_connection = sqlite3.connect(opensubs_db_path)
            # default: rows are tuples
            opensubs_db_connection.row_factory = sqlite3.Row # rows are dicts
            opensubs_db_cursor = opensubs_db_connection.cursor()
        opensubs_db_cursor.execute("select * from subz where num = (?)", (sub_number,))
        row = opensubs_db_cursor.fetchone()
        #print("row", repr(row)[0:200], "...")
        zip_name = row["name"][22:-1] # example: pulp.fiction.zip
        zip_basename = zip_name[0:-4] # example: pulp.fiction
        zip_path = f"{cached_zip_dir}/{sub_number:09d}.{zip_name}" # example: ../done-zips/000000123.pulp.fiction.zip
        cached_zip_files[sub_number] = zip_path
        # atomic write
        with open(zip_path + ".tmp", "wb") as f:
            f.write(row["file"])
        os.rename(zip_path + ".tmp", zip_path)

    # sub_dirname is not used
    # we just need sub_num_parts
    #sub_num_parts = "1cd" # SubSumCD in subtitles_all.txt
    # get the sub name
    # 10: remove padded sub number with dot. example: "000000001."
    # -4: remove ".zip" file extension
    sub_dirname = os.path.basename(zip_path)[10:-4]
    #print("sub_dirname", repr(sub_dirname))
    sub_num_parts = sub_dirname.split(".")[-2]
    assert re.fullmatch(r"\d+cd", sub_num_parts)

    if False:
        # get the sub name
        # 10: remove padded sub number with dot. example: "000000001."
        # -4: remove ".zip" file extension
        sub_dirname = os.path.basename(zip_path)[10:-4]
        print("sub_dirname", repr(sub_dirname))

        # os.path.basename(zip_path) # "000000001.alien.3.(1992).eng.2cd.(1).zip"
        # sub_dirname # "alien.3.(1992).eng.2cd.(1)"
        # movie_name_year_original # "alien.3.(1992)"
        # movie_name # "alien 3"
        # movie_year # "1992"

        sub_dirname_prefix = f"{movie_name_year_original}.{lang_code}." # alien.3.(1992).eng.
        print("sub_dirname_prefix", repr(sub_dirname_prefix))
        assert (
            sub_dirname.startswith(sub_dirname_prefix)
        ), (
            f"sub_dirname {repr(sub_dirname)} must start with {repr(sub_dirname_prefix)}"
        )

        sub_dirname_rest = sub_dirname[len(sub_dirname_prefix):] # 2cd.(1)
        print("sub_dirname_rest", repr(sub_dirname_rest))
        assert re.fullmatch(r"\d+cd\.\(\d+\)", sub_dirname_rest), f"sub_dirname_rest {repr(sub_dirname_rest)}"

        sub_dirname = f"{movie_name_year_original}.{lang_code}." # alien.3.(1992).eng.

        #group_name = f"""{movie_name.replace(" ", ".")}.{movie_year}.{lang_code}""" # alien.3.1992.eng
    if False:
        # simple
        # sub_dirname_prefix is in group_name
        # 1cd/2cd/... is "1cd" for 98% of all subs, so its not relevant
        # not used
        sub_dirname = str(sub_number)

    # TODO remove parens from year and sub_number
    # a: alien.3.(1992).eng.1cd.(114195)
    # b: alien.3.1992.eng.1cd.114195
    # TODO use a strict format for group_name. see also: index.txt.outliers
    # "strict" means: easy to parse
    # TODO remove ".1cd" ".2cd" etc. 98% of all subs are 1cd
    # a: alien.3.(1992).eng.1cd.(114195)
    # b: alien.3.1992.eng.114195
    # TODO? add prefix "os" before sub_number
    # a: alien.3.(1992).eng.1cd.(114195)
    # b: alien.3.1992.eng.os114195
    if False:
        # move the sub_number to the front of the dirname
        # a: kamikaze.girls.(2004).eng.1cd.(237742)
        # b: 237742.kamikaze.girls.(2004).eng.1cd
        # remove number suffix. example: ".(1234)"
        sub_dirname = sub_dirname[0:(-1 * len(f".({sub_number})"))]
        # add number prefix. example: "1234.". not zero-padded -> use numeric sort for tar
        sub_dirname = f"{sub_number}.{sub_dirname}"
        #print("sub_dirname", sub_dirname)

    # TODO remove the movie name prefix?
    # a: kamikaze.girls.(2004).eng.1cd.(237742)
    # b: 1cd.(237742)
    # no. keep it simple. just keep the original name

    # now sorting is useless because "1cd" comes before the sub number
    # example:
    # the.curse.of.the.jade.scorpion.(2001).eng.1cd.(109215)/
    # the.curse.of.the.jade.scorpion.(2001).eng.1cd.(96132)/

    # not used. see below (sub_tempdir)
    if False:
        sub_tempdir = f"{group_tempdir}/{sub_dirname}"
        os.makedirs(sub_tempdir) # exist_ok=True
        #print("sub_tempdir", sub_tempdir)

    with zipfile.ZipFile(zip_path, 'r') as sub_zip:
        # infolist: Return a list containing a ZipInfo object for each member of the archive.
        # The objects are in the same order as their entries in the actual ZIP file on disk if an existing archive was opened.
        # Note: The open(), read() and extract() methods can take a filename or a ZipInfo object.
        # You will appreciate this when trying to read a ZIP file that contains members with duplicate names.
        seen_filepaths = set()

        # TODO test

        # FIXME fix weird chars in filename?
        # alien.3.1992.eng/5980562/Alien┬│.en.srt
        # alien.3.1992.eng/6029152/Alien┬│.Directors.Cut.1992.1080p.BRrip.x264.GAZ.YIFY.srt

        # guess encoding of file names
        # Note: The ZIP file standard historically did not specify a metadata encoding,
        # but strongly recommended CP437 (the original IBM PC encoding) for interoperability.
        # Recent versions allow use of UTF-8 (only).
        metadata_encoding = magic.detect_from_content("\n".join(sub_zip.namelist())).encoding
        if metadata_encoding != "binary":
            print(f"sub {group_name} {sub_number}: TODO verify: metadata_encoding {metadata_encoding} from sub_zip.namelist", repr("\n".join(sub_zip.namelist())))

        # find the nfo file
        nfo_file_content = ""
        for zip_info in sub_zip.infolist():
            # TODO be more specific? see below: multiple nfo files
            if not zip_info.filename.endswith(f".({sub_number}).nfo"):
                continue
            if nfo_file_content != "":
                print(f"sub {group_name} {sub_number}: found rare edge-case: multiple nfo files")
                break
            with sub_zip.open(zip_info, "r") as src:
                nfo_file_content = src.read()
            while True:
                try:
                    nfo_file_content = nfo_file_content.decode("utf8")
                    break
                except UnicodeDecodeError as err:
                    print(
                        f"sub {sub_number}: found rare edge-case: "
                        f"invalid utf8 in the nfo file {repr(zip_info.filename)}: {err}. "
                        f"replacing broken bytes with null bytes"
                    )
                    # sub 8490893: found rare edge-case: invalid utf8 in the nfo file 'there.s.something.about.mary.(8490893).nfo': 'utf-8' codec can't decode byte 0xc2 in position 4037: invalid continuation byte. replacing broken bytes with null bytes
                    # replace broken bytes with null bytes
                    # later remove null bytes with rstrip("\x00")
                    nfo_file_content_2 = (
                        nfo_file_content[0:err.start] +
                        b"\x00" * (err.end - err.start) +
                        nfo_file_content[err.end:]
                    )
                    if False:
                        print(f"err.start: {err.start}")
                        print(f"err.end: {err.end}")
                        print(f"nfo_file_content at error +-4bytes: {repr(nfo_file_content[err.start - 4:err.end + 4])}")
                        print(f"nfo_file_content_2 at error +-4bytes: {repr(nfo_file_content[err.start - 4:err.end + 4])}")
                        print(f"nfo_file_content_2[err.start]: {repr(nfo_file_content_2[err.start])}")
                        print(f"nfo_file_content_2[err.start - 1]: {repr(nfo_file_content_2[err.start - 1])}")
                        print(f"nfo_file_content_2[err.start + 1]: {repr(nfo_file_content_2[err.start + 1])}")
                        assert len(nfo_file_content) == len(nfo_file_content_2)
                    nfo_file_content = nfo_file_content_2
                    if False:
                        print("""                    b"\\x00": """ + repr(b"\x00"))
                        print(f"nfo_file_content[err.start]: {repr(nfo_file_content[err.start])}")
                        print("""                    b"\\x00": """ + "%02x".format(b"\x00"))
                        print(f"nfo_file_content[err.start]: {nfo_file_content[err.start]:02x}")
                        print(f"nfo_file_content[err.start - 1]: {repr(nfo_file_content[err.start - 1])}")
                        print(f"nfo_file_content[err.start + 1]: {repr(nfo_file_content[err.start + 1])}")
                        print(f"nfo_file_content at error +-4bytes: {repr(nfo_file_content[err.start - 4:err.end + 4])}")
                    #assert nfo_file_content[err.start] == b"\x00"[0]
                    assert nfo_file_content[err.start] == 0

        # parse file names, sizes and hashes from the nfo file
        # the file names are cut after 57 bytes
        # note: 57 bytes, not 57 utf8 chars
        # this is a bug in the nfo generator
        # Û  Filename       : 123456789012345678901234567890123456789012345678901234567 Û
        zip_file_integrity_data = []
        nfo_file_filename = None
        class IntegrityData:
            # maximum length is 57 bytes
            name_57bytes = None
            size = None
            md5 = None
            # FIXME
            def __str__(self):
                return "str(dict(self))"
        integrity = IntegrityData()
        for line in nfo_file_content.split("\n"):
            # note: positions are measured in utf8 characters
            # line[0:1] == "Û"
            # line[0:1].encode("utf8") == b'\xc3\x9b'
            # bug in the opensubtitles nfo generator:
            # when the filename contains utf8 chars,
            # then its length is counted in bytes
            # when it should be counted in utf8 chars
            # so, when there is a two-byte utf8 char in the name,
            # the filename is cut 1 character too early
            # this can produce invalid utf8 chars
            # when the cut removes bytes from a utf8 char
            # example:
            # "ö".encode("utf8") == b'\xc3\xb6'
            # b'\xc3\xb6'[0:1].decode("utf8")
            # -> UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data
            if line.startswith("Û  Filename       : "):
                integrity.name_57bytes = line[20:-2].rstrip().rstrip("\x00")
            elif line.startswith("Û  Size           : "):
                size_str = line[20:-2].rstrip() # "12345 bytes"
                assert size_str.endswith(" bytes")
                size_str = size_str[0:-6]
                integrity.size = int(size_str)
            elif line.startswith("Û  MD5            : "):
                integrity.md5 = line[20:-2].rstrip().lower()
                assert len(integrity.md5) == 32
                zip_file_integrity_data.append(integrity)
                # next file
                integrity = IntegrityData()
        integrity = None
        #print("zip_file_integrity_data", zip_file_integrity_data) # debug

        for zip_info in sub_zip.infolist():
            # this fails when filename is empty
            # File "python3-3.10.9/lib/python3.10/zipfile.py", line 530, in is_dir
            # return self.filename[-1] == '/'
            #if zip_info.is_dir():
            # fixed in new version of python
            if zip_info.filename.endswith("/"):
                continue
            with sub_zip.open(zip_info, "r") as src:
                file_content = src.read()
            filepath = zip_info.filename

            if filepath.endswith(f".({sub_number}).nfo"):
                # remove the opensubtitles nfo files
                # all useful data (FPS, IMDB) is also stored in subtitles_all.txt
                # the nfo files also contain sizes and MD5 hashes of the subtitle files
                # these sizes and hashes can be useful to verify the orignal files
                # but we recode the files to utf8, so the hashes are different
                continue

            # é -> latin1 hex e9, dec 233
            # La cité des enfants perdus
            # sub 199799: weird filename '[58237] Cit.233; des enfants perdus, La (1995).srt'
            # sub 3305602: weird filename 'La cit├⌐ des enfants perdus.srt'
            # sub 3526667: weird filename 'La cit├⌐ des enfants perdus (The City of Lost Children) .srt'
            # https://stackoverflow.com/a/29351191/10440128
            # $ echo -n '├⌐' | hexdump -C
            # e2 94 9c e2 8c 90
            # $ echo -n 'é' | hexdump -C
            # c3 a9
            # $ printf "\xc3\xa9" | iconv -f cp437 -t utf8
            # ├⌐
            # $ printf 'é' | iconv -f cp437 -t utf8
            # ├⌐
            # $ printf '├⌐' | iconv -f utf8 -t cp437
            # é
            # https://stackoverflow.com/questions/13261347/correctly-decoding-zip-entry-file-names-cp437-utf-8-or
            # TODO allow more chars?
            if not re.fullmatch(r"""[a-zA-Z0-9 \[\](){}.,;:_?!'"~+-]*""", filepath):
                done_fix = False
                for filepath_encoding in ["cp437", "iso-8859-1"]:
                    try:
                        # '├⌐'.encode("cp437").decode("utf8") == 'é'
                        fixed_filepath = filepath.encode(filepath_encoding).decode("utf8")
                    except UnicodeEncodeError:
                        continue
                    print(f"sub {group_name} {sub_number}: recoding file name from {filepath_encoding} to utf8: from {repr(filepath)} to {repr(fixed_filepath)}")
                    filepath = fixed_filepath
                    done_fix = True
                    break
                if not done_fix:
                    print(f"sub {group_name} {sub_number}: keeping weird file name {repr(filepath)}")

            # verify file name and content
            if False:
                filepath_57bytes = None
                try:
                    filepath_57bytes = filepath.encode("utf8")[0:57].decode("utf8")
                except UnicodeDecodeError as err:
                    print(
                        f"sub {sub_number}: found rare edge-case: "
                        f"cutting the file name {repr(filepath)} after 57 bytes produces invalid utf8: {err}. "
                        f"this means we currently cannot validate the file name, "
                        f"and probably we cannot read the nfo file"
                    )
            # filepath_57bytes is not reliable to find all files
            # so we find files by size and hash
            done_file_integrity = False
            file_content_md5 = hashlib.md5(file_content).hexdigest()
            file_content_len = len(file_content)
            for integrity in zip_file_integrity_data:
                if not (
                    file_content_len == integrity.size and
                    file_content_md5 == integrity.md5
                ):
                    continue
                #if filepath_57bytes and not filepath_57bytes == integrity.name_57bytes:
                # integrity.name_57bytes can be shorter because rstrip("\x00"), so compare prefix
                if not filepath.startswith(integrity.name_57bytes):
                    print(
                        f"sub {sub_number}: failed to verify file name {repr(filepath)}. "
                        f"actual filepath {repr(filepath)}. "
                        f"expected prefix {repr(integrity.name_57bytes)}"
                    )
                done_file_integrity = True
                break
            if not done_file_integrity:
                print(
                    f"sub {sub_number}: failed to verify file content {repr(filepath)}. "
                    f"actual size {file_content_len}. "
                    f"actual md5 {file_content_md5}"
                )

            # cleanup filename
            # TODO be more conservative and keep filenames?
            # usually we dont care at all about filenames
            # its more important to have correct file extensions
            # https://www.ascii-code.com/
            filepath = filepath.replace("\xa0", " ") # \xa0 = &nbsp; = Non-breaking space
            # https://en.wikipedia.org/wiki/Quotation_mark#Unicode_code_point_table
            # replace fancy single quotes with apostrophe = &apos; = straight single quote
            filepath = filepath.replace("´", "'") # acute accent
            filepath = filepath.replace("‘", "'") # left single quotation mark
            filepath = filepath.replace("’", "'") # right single quotation mark
            filepath = filepath.replace("‚", "'") # single low-9 quotation mark
            filepath = filepath.replace("‛", "'") # single high-reversed-9 quotation mark
            filepath = filepath.replace("＇", "'") # fullwidth apostrophe
            # replace fancy double quotes with quotation mark = &quot; = straight double quote
            filepath = filepath.replace("“", '"') # left double quotation mark
            filepath = filepath.replace("”", '"') # right double quotation mark
            filepath = filepath.replace("„", '"') # double low-9 quotation mark
            filepath = filepath.replace("‟", '"') # double high-reversed-9 quotation mark
            filepath = filepath.replace("⹂", '"') # double low-reversed-9 quotation mark
            filepath = filepath.replace("＂", '"') # fullwidth quotation mark

            # fix empty file name
            #print(f"sub {group_name} {sub_number}: zip_info.filename:", repr(zip_info.filename))
            if filepath == "":
                # example: 237742.kamikaze.girls.(2004).eng.1cd
                # the file with empty filename is a SAMI file
                # https://en.wikipedia.org/wiki/SAMI
                # magic_result.mime_type = "text/html"
                # TODO also use SubFormat from subtitles_all/subtitles_all.txt.gz-parse-result.txt
                # here:
                # {"IDSubtitle": 237742, "MovieName": "Kamikaze Girls", "MovieYear": 2004, "LanguageName": "English", "ISO639": "en", "SubAddDate": "2006-03-23 00:00:00", "ImdbID": 416220, "SubFormat": "smi", "SubSumCD": 1, "MovieReleaseName": "Shimotsuma Monogatari (2004)", "MovieFPS": 0.0, "SeriesSeason": 0, "SeriesEpisode": 0, "SeriesIMDBParent": 0, "MovieKind": "movie", "URL": "http://www.opensubtitles.org/subtitles/237742/kamikaze-girls-en"}
                # "SubFormat": "smi"
                # "smi" is short for "sami"
                # "smi" file extension is ambiguous, so "sami" is better
                print(f"sub {group_name} {sub_number}: empty filename in zip_path {repr(zip_path)}. using zip_basename {repr(zip_basename)}")
                if False:
                    print(f"sub {group_name} {sub_number}: zip_path", repr(zip_path))
                    print(f"sub {group_name} {sub_number}: zip_name", repr(zip_name))
                    print(f"sub {group_name} {sub_number}: zip_basename", repr(zip_basename))
                    raise NotImplementedError
                filepath = zip_basename
                magic_result = magic.detect_from_content(file_content)
                print(f"sub {group_name} {sub_number}: empty filename in zip_path {repr(zip_path)}. magic_result {repr(magic_result)}")
                if magic_result.mime_type.startswith("text/html"):
                    if re.search(b"<sami>", file_content, re.I):
                        filepath += ".sami"
                    else:
                        filepath += ".html"
                elif magic_result.mime_type.startswith("text/"):
                    if is_sub_content(file_content):
                        filepath += ".sub"
                    else:
                        filepath += ".txt"
                else:
                    #filepath += ".unknown"
                    print(f"sub {group_name} {sub_number}: empty filename in zip_path {repr(zip_path)}. magic_result {repr(magic_result)} TODO add file extension")
                    raise NotImplementedError
                print(f"sub {group_name} {sub_number}: empty filename in zip_path {repr(zip_path)}. using filepath {repr(filepath)}")
            # fix wrong file extension
            elif filepath[-4:].lower() == ".txt":
                if is_sub_content(file_content):
                    filepath = filepath[0:-4] + ".sub"
                    print(f"sub {group_name} {sub_number}: renaming txt file to sub file {repr(filepath)}")

            # convert file path to utf8
            if convert_to_utf8:
                if metadata_encoding not in {"us-ascii", "utf-8", "unknown-8bit", "binary"}:
                    # TODO test
                    print(f"sub {group_name} {sub_number}: converting file path from {metadata_encoding} to utf8: {repr(filepath)}")
                    filepath = filepath.decode(metadata_encoding).encode("utf8") # bytes -> str -> bytes

            # fix file extension
            filepath_base, filename_extension = os.path.splitext(filepath)
            # based on subtitles_all.txt.gz-parse.py
            expr_filename_extension = r"\.(srt|sub|txt|mpl|smi|ssa|ass|tmp|vtt)"
            if not re.fullmatch(expr_filename_extension, filename_extension.lower()):
                if is_sub_content(file_content):
                    filepath += ".sub"
                    print(f"sub {group_name} {sub_number}: adding sub file extension to file name {repr(filepath)}")
                else:
                    print(f"sub {group_name} {sub_number}: FIXME no file extension in file name {repr(filepath)}")
                    # example: sub 62421: 'Magnolia (DVDRip - DivX5.02 - dual English . Espa'
                    # ". Espa" is not a file extension
                    # sub 193348: FIXME no file extension in file name 'Bez ko'
                    # sub 2364: FIXME no file extension in file name '[1965] Luis Bunuel - Sim'
                    # sub 3458687: FIXME no file extension in file name 'Le Charme Discret de la Bourgeoisie [Discreet Charm of the Bourgeoisie] (1972, Buńuel) [DVDivX-SC.sr'
                    # sub 2371: FIXME no file extension in file name '[1974] Luis Bunuel - Le Fant'
                    # sub 2373: FIXME no file extension in file name '[1977] Luis Bunuel - Cet obscur objet du d'
                    # sub 89545: FIXME no file extension in file name 'La.'
                    # sub 4019866: FIXME no file extension in file name '1993 - Martin Scorsese - The age of innocence - Con Daniel Day Lewis, Michelle Pfeiffer, Winona Ryde'
                    # sub 3086906: FIXME no file extension in file name 'Antonioni, Michelangelo . Wenders, Wim - Al Di Là Delle Nuvole - DivX 3 x 512 x 304 . 23,976 - Audio'
                    # sub 2432: FIXME no file extension in file name '[1967] Milos Forman - Ho'
                    # sub 3110428: FIXME no file extension in file name 'Mikhalkov, Nikita - (1979) - Neskolko dnej iz zhizni I.I. Oblomova - CD 2 - XviD - 540 x 416 . 25 -'
                    # sub 3557338: FIXME no file extension in file name '[HDteam] Austin.Powers.La.Espia.que.me.Achucho.1999.[1080p].[BluRay].[x264].[Dual].[Subs]-by Ruben.E'
                    # sub 114261: FIXME no file extension in file name 'contact_eng'
                    # sub 3103577: FIXME no file extension in file name 'Kubrick, Stanley - (1960) - Spartacus - CD 1 - DivX 5 - 576 x 240 . 25 - English - Separate Subtitle'
                    # sub 4089080: FIXME no file extension in file name 'srt'
                    # sub 3377354: FIXME no file extension in file name "The Bishop's Wife (1947) Henry Koster (Cary Grant Loretta Young David Niven) Gregg Toland6 engish.sr"
                    # sub 3321622: FIXME no file extension in file name 'Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl.Remastered.2003.720p.BluRay.DTSx264-ESiR.eng.s'
                    # sub 136869: FIXME no file extension in file name 'Pirates of the cara'

            # rename duplicate files
            done_rename = False
            old_filepath = filepath
            old_filepath_base, filename_extension = os.path.splitext(filepath)
            if not re.fullmatch(expr_filename_extension, filename_extension.lower()):
                old_filepath_base, filename_extension = filepath, ""
            duplicate_number = 1
            while filepath in seen_filepaths:
                # TODO check for duplicate file_content
                # if file exists with exact same file_content, then keep only the first file
                # files can differ only in encoding
                # for example one file in utf8, other file in latin1
                # file numbers are one-based:
                # 1 = original file
                # 2 = duplicate 1
                # 3 = duplicate 2
                duplicate_number += 1
                filepath = f"{old_filepath_base}.{duplicate_number}{filename_extension}"
                done_rename = True
            seen_filepaths.add(filepath)
            if done_rename:
                print(f"sub {group_name} {sub_number}: renaming duplicate file from {repr(old_filepath)} to {repr(filepath)}")
                # FIXME? sub 62421: renaming duplicate file from 'Magnolia (DVDRip - DivX5.02 - dual English . Espa' to 'Magnolia (DVDRip - DivX5.02 - dual English .2. Espa'

            #print(f"sub {group_name} {sub_number}: filepath", filepath)
            # extract cannot change the filename
            #sub_zip.extract(zip_info, path=filepath) # pwd=sub_tempdir.encode("utf8") # has no effect?
            filepath_dirname = os.path.dirname(filepath)
            if filepath_dirname != "":
                os.makedirs(filepath_dirname, exist_ok=True)

            # TODO repack old groups with old filepath_prefix format
            # see logfile: new format starts at
            #   group the.amityville.horror.1979.eng
            #   sub 50324: sub_num_parts '2cd'
            filepath_prefix = f"{sub_number}."
            if sub_num_parts != "1cd":
                #print(f"sub {group_name} {sub_number}: sub_num_parts {repr(sub_num_parts)}")
                filepath_prefix = f"{sub_number}-{sub_num_parts}."

            # dont use sub_tempdir
            # 1cd-subs (98% of all subs) contain only one file
            # having only one file per folder is a waste of inodes
            # assuming that on extraction, the archive's file layout is used
            # so we put all files in group_tempdir
            # and we prefix all files with sub_number
            #with open(f"{sub_tempdir}/{filepath}", "wb") as dst:
            with open(f"{group_tempdir}/{filepath_prefix}{filepath}", "wb") as dst:
                if convert_to_utf8:
                    content_encoding = magic.detect_from_content(file_content).encoding
                    if content_encoding not in {"us-ascii", "utf-8", "unknown-8bit", "binary"}:
                        # convert file file_content to utf8
                        # TODO test
                        if verbose_recode_content:
                            print(f"sub {group_name} {sub_number}: recoding file content from {content_encoding} to utf8: {repr(filepath)}")
                        file_content = file_content.decode(content_encoding).encode("utf8") # bytes -> str -> bytes
                dst.write(file_content)


main()
